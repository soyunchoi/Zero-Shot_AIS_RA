"""
VLM-SAM í†µí•© ëª¨ë¸: LLaVAë¥¼ ì‚¬ìš©í•œ Occlusion ê´€ê³„ ì¶”ì¶œ ë° SAMì„ í™œìš©í•œ ë§ˆìŠ¤í¬ ì˜ˆì¸¡
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import copy
import json
import re
import os
from typing import Dict, List, Tuple, Optional
from PIL import Image
import numpy as np

# EfficientSAM ëª¨ë“ˆ ì„í¬íŠ¸
from EfficientSAM.efficient_sam.build_efficient_sam import build_efficient_sam_vitt

# LLaVA ê´€ë ¨ ì„í¬íŠ¸ (main_prompt_VLM_reasoning_250704.py ë°©ì‹)
try:
    from transformers import AutoProcessor, LlavaForConditionalGeneration
    import torch
    
    # ì„±ê³µì ìœ¼ë¡œ ê²€ì¦ëœ ë°©ì‹ ì‚¬ìš©
    LLAVA_MODEL_CLASS = LlavaForConditionalGeneration
    print("âœ“ LlavaForConditionalGeneration ì‚¬ìš© (ê²€ì¦ëœ ë°©ì‹)")
    
    print("âœ“ Transformers ëª¨ë“ˆ ì„í¬íŠ¸ ì„±ê³µ")
    
    # LLaVA ëª¨ë¸ ì„¤ì • (ê²€ì¦ëœ ëª¨ë¸ ì‚¬ìš©)
    DEFAULT_LLAVA_MODEL = "llava-hf/llava-1.5-7b-hf"  # ì„±ê³µì ìœ¼ë¡œ ê²€ì¦ëœ ëª¨ë¸
    
    print(f"âœ“ LLaVA ëª¨ë¸ ì„¤ì •: {DEFAULT_LLAVA_MODEL}")
    print(f"  (HuggingFace Hubì—ì„œ ìë™ ë‹¤ìš´ë¡œë“œ ë° ìºì‹œë©ë‹ˆë‹¤)")
    LLAVA_AVAILABLE = True
        
except ImportError as e:
    print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ë¥¼ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")
    print("VLM-SAM ëª¨ë¸ì€ transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìˆ˜ì…ë‹ˆë‹¤.")
    print("pip install transformersë¡œ ì„¤ì¹˜í•˜ì„¸ìš”.")
    raise ImportError(f"Transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ê°€ í•„ìš”í•˜ì§€ë§Œ ì„í¬íŠ¸í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤: {e}")

# Attention Extractor ì„í¬íŠ¸
from attention_extractor import AttentionExtractor

# Point Sampler ì„í¬íŠ¸
from point_sampler import AttentionPointSampler

# Integrated Visualizer ì„í¬íŠ¸
from integrated_visualizer import IntegratedVisualizer

# D2SA Dataset ì„í¬íŠ¸
from d2sa_dataset import D2SADataset
from torch.utils.data import DataLoader
from torchvision import transforms


class OcclusionAnalyzer:
    """VLMì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ occlusion ê´€ê³„ë¥¼ ë¶„ì„í•˜ëŠ” í´ë˜ìŠ¤"""
    
    def __init__(self, model_path=None):
        """
        Args:
            model_path (str): ì‚¬ìš©í•  LLaVA ëª¨ë¸ ê²½ë¡œ (Noneì´ë©´ ê¸°ë³¸ ë¡œì»¬ ê²½ë¡œ ì‚¬ìš©)
        """
        self.device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
        
        # ëª¨ë¸ ê²½ë¡œ ì„¤ì •
        if model_path is None:
            model_path = DEFAULT_LLAVA_MODEL
        
        print(f"ğŸ”„ LLaVA ëª¨ë¸ ë¡œë”© ì‹œì‘: {model_path}")
        
        # LLaVA ëª¨ë¸ ë¡œë”© (í•„ìˆ˜) - ì—¬ëŸ¬ ëª¨ë¸ ì‹œë„
        success = False
        last_error = None
        
        # ë‹¨ì¼ ëª¨ë¸ ì‹œë„ (ê²€ì¦ëœ ëª¨ë¸ ì‚¬ìš©)
        try:
            success = self._try_load_model(model_path)
        except Exception as e:
            last_error = e
        
        if not success:
            print(f"âŒ ì¹˜ëª…ì  ì˜¤ë¥˜: ëª¨ë“  LLaVA ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨")
            print("ê°€ëŠ¥í•œ í•´ê²° ë°©ë²•:")
            print("1. ì¸í„°ë„· ì—°ê²° í™•ì¸")
            print("2. HuggingFace ì ‘ê·¼ ê¶Œí•œ í™•ì¸")
            print("3. ì¶©ë¶„í•œ ë©”ëª¨ë¦¬ (RAM/VRAM) í™•ì¸")
            print("4. transformers ë¼ì´ë¸ŒëŸ¬ë¦¬ ì—…ë°ì´íŠ¸: pip install --upgrade transformers")
            raise RuntimeError(f"ëª¨ë“  LLaVA ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨. ë§ˆì§€ë§‰ ì˜¤ë¥˜: {last_error}")
        
        # í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™”
        self.__init_prompts__()
    
    def _try_load_model(self, model_path: str) -> bool:
        """ê°œë³„ ëª¨ë¸ ë¡œë“œ ì‹œë„"""
        try:
            # Processor ë¡œë“œ (ê²€ì¦ëœ ë°©ì‹)
            print("ğŸ“¦ LLaVA Processor ë¡œë“œ ì¤‘...")
            self.processor = AutoProcessor.from_pretrained(
                model_path, 
                use_fast=False  # main_prompt_VLM_reasoning_250704.pyì™€ ë™ì¼í•œ ì„¤ì •
            )
            print("âœ“ LLaVA Processor ë¡œë“œ ì™„ë£Œ")
            
            # Model ë¡œë“œ (ê²€ì¦ëœ ë°©ì‹)
            print("ğŸ¤– LLaVA Model ë¡œë“œ ì¤‘...")
            self.model = LLAVA_MODEL_CLASS.from_pretrained(model_path).to(self.device)
            print(f"âœ“ LLaVA ëª¨ë¸ ë¡œë“œ ì™„ë£Œ: {model_path}")
            print(f"âœ“ ë””ë°”ì´ìŠ¤: {self.device}")
            
            # Attention Extractor ì´ˆê¸°í™”
            self.attention_extractor = AttentionExtractor(self.model, self.processor)
            print("âœ“ Attention Extractor ì´ˆê¸°í™” ì™„ë£Œ")
            
            # Point Sampler ì´ˆê¸°í™”
            self.point_sampler = AttentionPointSampler()
            print("âœ“ Point Sampler ì´ˆê¸°í™” ì™„ë£Œ")
            
            return True
            
        except Exception as e:
            print(f"âŒ ëª¨ë¸ ë¡œë“œ ì‹¤íŒ¨ ({model_path}): {e}")
            return False
    
    def __init_prompts__(self):
        """í”„ë¡¬í”„íŠ¸ í…œí”Œë¦¿ ì´ˆê¸°í™” (ìƒì„±ì ëì—ì„œ í˜¸ì¶œ)"""
        self.occlusion_prompt = (
            "USER: <image>Analyze the image for occlusion relationships.\n"
            "1. Visible Objects: List all clearly visible objects in the scene using basic object names (e.g., 'car', 'person', 'chair'). Separate by commas.\n"
            "2. Occluded Objects: List objects that are partially hidden by other objects. Use basic object names. If none, state 'None'.\n"
            "3. Occlusion Relationships: For each occluded object, identify what is occluding it.\n\n"
            "Respond strictly in this format:\n"
            "VISIBLE_OBJECTS: [answer to 1]\n"
            "OCCLUDED_OBJECTS: [answer to 2]\n"
            "RELATIONSHIPS: [answer to 3]\n"
            "ASSISTANT:"
        )

    def analyze_occlusion(self, image: Image.Image) -> Dict:
        """
        ì´ë¯¸ì§€ì—ì„œ occlusion ê´€ê³„ë¥¼ ë¶„ì„í•©ë‹ˆë‹¤.
        
        Args:
            image (PIL.Image): ë¶„ì„í•  ì´ë¯¸ì§€
            
        Returns:
            Dict: occlusion ê´€ê³„ ì •ë³´
        """
        # LLaVA ëª¨ë¸ì´ í•„ìˆ˜ì´ë¯€ë¡œ í•­ìƒ ì¡´ì¬í•´ì•¼ í•¨
        assert self.model is not None and self.processor is not None, "LLaVA ëª¨ë¸ì´ ë¡œë“œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!"
        
        try:
            # main_prompt_VLM_reasoning_250704.pyì™€ ë™ì¼í•œ ë°©ì‹
            inputs = self.processor(
                text=self.occlusion_prompt, 
                images=image, 
                return_tensors="pt"
            ).to(self.device)
            
            # ëª¨ë¸ ì¶”ë¡  (main_prompt_VLM_reasoning_250704.pyì™€ ë™ì¼í•œ ë°©ì‹)
            with torch.no_grad():
                generate_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=200,  # ê²€ì¦ëœ ì„¤ì •
                    temperature=0.0     # ê²€ì¦ëœ ì„¤ì •
                )
            
            # ì‘ë‹µ ë””ì½”ë”©
            response = self.processor.batch_decode(
                generate_ids, 
                skip_special_tokens=True, 
                clean_up_tokenization_spaces=False
            )[0]
            
            # LLaVA ì‘ë‹µ íŒŒì‹±
            occlusion_data = self._parse_llava_response(response)
            
            return occlusion_data
            
        except Exception as e:
            print(f"âŒ Occlusion ë¶„ì„ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            print("LLaVA ëª¨ë¸ì„ ì‚¬ìš©í•œ VLM ë¶„ì„ì´ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            # ì‹¤ì œ VLM ë¶„ì„ ì‹¤íŒ¨ ì‹œ ì˜ˆì™¸ ë°œìƒ
            raise RuntimeError(f"VLM ë¶„ì„ ì‹¤íŒ¨: {e}")
    
    def extract_attention_maps(self, image: Image.Image, prompt: str = None, use_vlsam_method: bool = True) -> Tuple[Dict, np.ndarray]:
        """
        VLMì—ì„œ attention mapì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            image (PIL.Image): ë¶„ì„í•  ì´ë¯¸ì§€
            prompt (str): ì‚¬ìš©í•  í”„ë¡¬í”„íŠ¸ (Noneì´ë©´ ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì‚¬ìš©)
            use_vlsam_method (bool): VL-SAM ë…¼ë¬¸ ë°©ì‹ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            Tuple[Dict, np.ndarray]: (layerë³„ attention maps, ì§‘ê³„ëœ attention map)
        """
        # LLaVA ëª¨ë¸ì´ í•„ìˆ˜ì´ë¯€ë¡œ attention_extractorê°€ í•­ìƒ ì¡´ì¬í•´ì•¼ í•¨
        assert self.attention_extractor is not None, "Attention Extractorê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!"
        
        try:
            # ê¸°ë³¸ í”„ë¡¬í”„íŠ¸ ì„¤ì •
            if prompt is None:
                prompt = "Analyze the objects in this image and their spatial relationships."
            
            print(f"ğŸ§  VLM Attention Map ì¶”ì¶œ ì¤‘ ({'VL-SAM ë°©ì‹' if use_vlsam_method else 'ê¸°ë³¸ ë°©ì‹'})...")
            
            # VL-SAM ë°©ì‹ ë˜ëŠ” ê¸°ë³¸ ë°©ì‹ ì„ íƒ
            raw_attention_maps = self.attention_extractor.extract_attention_maps(
                image, prompt, use_vlsam_method=use_vlsam_method
            )
            
            if not raw_attention_maps:
                print("âŒ Attention map ì¶”ì¶œ ì‹¤íŒ¨")
                raise RuntimeError("VLMì—ì„œ attention mapì„ ì¶”ì¶œí•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            
            # í›„ì²˜ë¦¬
            processed_maps = self.attention_extractor.process_attention_maps(
                raw_attention_maps, 
                image_size=(1024, 1024)
            )
            
            # ì§‘ê³„
            aggregated_map = self.attention_extractor.aggregate_attention_maps(processed_maps)
            
            print(f"âœ“ Attention Map ì¶”ì¶œ ì™„ë£Œ: {len(processed_maps)}ê°œ layer")
            
            return processed_maps, aggregated_map
            
        except Exception as e:
            print(f"âŒ VLM Attention Map ì¶”ì¶œ ì¤‘ìš” ì‹¤íŒ¨!")
            print(f"   ì˜¤ë¥˜ ë‚´ìš©: {e}")
            print(f"âŒ ì‹¤ì œ VLM attention mapì„ ì‚¬ìš©í•´ì•¼ í•˜ì§€ë§Œ ì¶”ì¶œì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤.")
            print(f"âŒ ì‹¤ì œ VLM attention mapì„ ì‚¬ìš©í•´ì•¼ í•˜ë¯€ë¡œ í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"VLM Attention Map ì¶”ì¶œ ì¤‘ìš” ì‹¤íŒ¨: {e}")
    
    def generate_sam_prompts(self, image: Image.Image, attention_maps: Dict, aggregated_attention: np.ndarray, 
                           bbox: np.ndarray = None, target_class: str = None, use_vlsam_method: bool = True) -> Tuple[np.ndarray, np.ndarray]:
        """
        Attention mapì—ì„œ SAM promptìš© pointë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            image: PIL ì´ë¯¸ì§€
            attention_maps: Layerë³„ attention maps
            aggregated_attention: ì§‘ê³„ëœ attention map
            bbox: ë°”ìš´ë”© ë°•ìŠ¤ [x1, y1, x2, y2]
            target_class: íƒ€ê²Ÿ ê°ì²´ í´ë˜ìŠ¤
            use_vlsam_method: VL-SAM ë…¼ë¬¸ ë°©ì‹ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            Tuple[np.ndarray, np.ndarray]: (points, labels) - SAM prompt í˜•ì‹
        """
        # LLaVA ëª¨ë¸ì´ í•„ìˆ˜ì´ë¯€ë¡œ point_samplerê°€ í•­ìƒ ì¡´ì¬í•´ì•¼ í•¨
        assert self.point_sampler is not None, "Point Samplerê°€ ì´ˆê¸°í™”ë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤!"
        
        try:
            print(f"ğŸ¯ SAM Prompt ìƒì„± ì¤‘ ({'VL-SAM ë°©ì‹' if use_vlsam_method else 'ì ì‘ì  ë°©ì‹'})...")
            
            if use_vlsam_method:
                # VL-SAM ë…¼ë¬¸ ë°©ì‹ ì‚¬ìš©
                positive_points, negative_points, labels = self.point_sampler.sample_points_from_attention(
                    aggregated_attention,
                    bbox=bbox,
                    use_vlsam_method=True
                )
            else:
                # ê¸°ì¡´ ì ì‘ì  point sampling ì‚¬ìš©
                positive_points, negative_points, labels = self.point_sampler.adaptive_point_sampling(
                    aggregated_attention,
                    bbox=bbox,
                    target_object_class=target_class
                )
            
            # SAM í˜•ì‹ìœ¼ë¡œ ë³€í™˜ (ëª¨ë“  í¬ì¸íŠ¸ ê²°í•©)
            if len(positive_points) > 0 and len(negative_points) > 0:
                all_points = np.concatenate([positive_points, negative_points], axis=0)
                all_labels = np.concatenate([
                    np.ones(len(positive_points), dtype=int),
                    np.zeros(len(negative_points), dtype=int)
                ], axis=0)
            elif len(positive_points) > 0:
                all_points = positive_points
                all_labels = np.ones(len(positive_points), dtype=int)
            elif len(negative_points) > 0:
                all_points = negative_points
                all_labels = np.zeros(len(negative_points), dtype=int)
            else:
                # Fallback: ë°”ìš´ë”© ë°•ìŠ¤ ì¤‘ì‹¬ì  ì‚¬ìš©
                if bbox is not None:
                    center_x = (bbox[0] + bbox[2]) // 2
                    center_y = (bbox[1] + bbox[3]) // 2
                    all_points = np.array([[center_x, center_y]])
                    all_labels = np.array([1])
                else:
                    all_points = np.array([[512, 512]])
                    all_labels = np.array([1])
            
            print(f"âœ“ SAM Prompt ìƒì„± ì™„ë£Œ: {len(all_points)}ê°œ í¬ì¸íŠ¸ ({np.sum(all_labels)}ê°œ positive)")
            
            return all_points, all_labels
            
        except Exception as e:
            print(f"âŒ SAM Prompt ìƒì„± ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"VLM ê¸°ë°˜ SAM Prompt ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _parse_llava_response(self, response: str) -> Dict:
        print(f"\n===== LLaVA Raw Response =====")
        print(response)
        print("==============================\n")
        
        # ê¸°ë³¸ê°’ ì´ˆê¸°í™”
        visible_objects = []
        occluded_objects = []
        occlusion_relationships = []
        
        try:
            # ASSISTANT ë¶€ë¶„ ì¶”ì¶œ
            assistant_response = response.split("ASSISTANT:")[-1].strip()
            
            # ê° ì„¹ì…˜ íŒŒì‹±
            import re
            
            visible_match = re.search(r"VISIBLE_OBJECTS:(.*?)(?=OCCLUDED_OBJECTS:|$)", assistant_response, re.IGNORECASE | re.DOTALL)
            occluded_match = re.search(r"OCCLUDED_OBJECTS:(.*?)(?=RELATIONSHIPS:|$)", assistant_response, re.IGNORECASE | re.DOTALL)
            relationships_match = re.search(r"RELATIONSHIPS:(.*)", assistant_response, re.IGNORECASE | re.DOTALL)
            
            if visible_match:
                visible_str = visible_match.group(1).strip()
                if visible_str and 'none' not in visible_str.lower():
                    visible_objects = [obj.strip() for obj in visible_str.split(',') if obj.strip()]
            
            if occluded_match:
                occluded_str = occluded_match.group(1).strip()
                if occluded_str and 'none' not in occluded_str.lower():
                    occluded_objects = [obj.strip() for obj in occluded_str.split(',') if obj.strip()]
            
            if relationships_match:
                relationships_str = relationships_match.group(1).strip()
                if relationships_str and 'none' not in relationships_str.lower():
                    # "A is occluded by B" í˜•ì‹ íŒŒì‹±
                    for relationship in relationships_str.split(';'):
                        relationship = relationship.strip()
                        if 'is occluded by' in relationship.lower():
                            parts = relationship.lower().split('is occluded by')
                            if len(parts) == 2:
                                occluded = parts[0].strip()
                                occluder = parts[1].strip()
                                occlusion_relationships.append({
                                    "occluded": occluded,
                                    "occluder": occluder
                                })
            
        except Exception as e:
            print(f"ì‘ë‹µ íŒŒì‹± ì¤‘ ì˜¤ë¥˜: {e}")
        
        # ê²°ê³¼ êµ¬ì„±
        result = {
            "visible_objects": visible_objects,
            "occluded_objects": occluded_objects,
            "occlusion_relationships": occlusion_relationships
        }
        
        print(f"íŒŒì‹± ê²°ê³¼: {result}")
        return result
    
    def _parse_response(self, response: str) -> Dict:
        """LLaVA ì‘ë‹µì—ì„œ JSON ë°ì´í„°ë¥¼ ì¶”ì¶œí•˜ì—¬ íŒŒì‹±í•©ë‹ˆë‹¤."""
        try:
            # JSON ë¸”ë¡ ì°¾ê¸°
            json_match = re.search(r'\{.*\}', response, re.DOTALL)
            if json_match:
                json_str = json_match.group()
                occlusion_data = json.loads(json_str)
                return occlusion_data
            else:
                print("âŒ ì‘ë‹µì—ì„œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                raise ValueError("LLaVA ì‘ë‹µì—ì„œ ìœ íš¨í•œ JSONì„ ì°¾ì„ ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
                
        except json.JSONDecodeError as e:
            print(f"âŒ JSON íŒŒì‹± ì˜¤ë¥˜: {e}")
            raise ValueError(f"LLaVA ì‘ë‹µì˜ JSON íŒŒì‹± ì‹¤íŒ¨: {e}")
    


class VLMSAMModel(nn.Module):
    """
    VLM (LLaVA)ê³¼ SAMì„ í†µí•©í•œ Zero-shot Amodal Instance Segmentation ëª¨ë¸
    
    1. VLMìœ¼ë¡œ occlusion ê´€ê³„ ë¶„ì„
    2. VLM decoderì—ì„œ attention map ìƒì„±  
    3. Attention mapì„ point samplingìœ¼ë¡œ ë³€í™˜
    4. SAMìœ¼ë¡œ amodal/visible ë§ˆìŠ¤í¬ ì˜ˆì¸¡
    5. D2SA ë°ì´í„°ì…‹ ì§ì ‘ í†µí•©
    """
    
    def __init__(self, llava_model_path=None, use_d2sa=True):
        super().__init__()
        
        print("=== VLM-SAM í†µí•© ëª¨ë¸ ì´ˆê¸°í™” ì‹œì‘ ===")
        
        # 1. Occlusion Analyzer ì´ˆê¸°í™” (ë¡œì»¬ LLaVA ëª¨ë¸ ì‚¬ìš©)
        self.occlusion_analyzer = OcclusionAnalyzer(llava_model_path)
        print("âœ“ Occlusion Analyzer ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 2. EfficientSAM ëª¨ë¸ ë¡œë“œ
        self.efficient_sam = build_efficient_sam_vitt().eval()
        print("âœ“ EfficientSAM (ViT-Tiny) ëª¨ë¸ ë¡œë“œ ì™„ë£Œ")

        # 3. SAM ì»´í¬ë„ŒíŠ¸ ë¶„ë¦¬
        self.image_encoder = self.efficient_sam.image_encoder
        self.prompt_encoder = self.efficient_sam.prompt_encoder
        
        # 4. íƒœìŠ¤í¬ë³„ íŠ¹í™”ëœ ë””ì½”ë” ìƒì„±
        self.amodal_mask_decoder = self._create_amodal_decoder()
        self.visible_mask_decoder = self._create_visible_decoder()
        print("âœ“ Amodal ë° Visible ë§ˆìŠ¤í¬ë¥¼ ìœ„í•œ íƒœìŠ¤í¬ë³„ íŠ¹í™” ë””ì½”ë” ìƒì„± ì™„ë£Œ")
        
        # 5. ëª¨ë“  SAM íŒŒë¼ë¯¸í„° ê³ ì • (zero-shotì´ë¯€ë¡œ í•™ìŠµ ì—†ìŒ)
        self._freeze_sam_parameters()
        print("âœ“ SAM íŒŒë¼ë¯¸í„° ê³ ì • ì™„ë£Œ (Zero-shot ëª¨ë“œ)")
        
        # 6. Integrated Visualizer ì´ˆê¸°í™”
        self.visualizer = IntegratedVisualizer()
        print("âœ“ Integrated Visualizer ì´ˆê¸°í™” ì™„ë£Œ")
        
        # 7. D2SA ë°ì´í„°ì…‹ ì´ˆê¸°í™” (ì˜µì…˜)
        self.use_d2sa = use_d2sa
        self.d2sa_dataset = None
        self.d2sa_dataloader = None
        
        if self.use_d2sa:
            self._initialize_d2sa_dataset()
        
        print("=== VLM-SAM í†µí•© ëª¨ë¸ ì´ˆê¸°í™” ì™„ë£Œ ===\n")

    def _create_amodal_decoder(self):
        """Amodal ë§ˆìŠ¤í¬ì— íŠ¹í™”ëœ ë””ì½”ë”"""
        decoder = copy.deepcopy(self.efficient_sam.mask_decoder)
        
        # Amodal íŠ¹í™”: ë” ë„“ì€ ì˜ì—­ì„ ì˜ˆì¸¡í•˜ë„ë¡ ì´ˆê¸°í™”
        for name, param in decoder.named_parameters():
            if 'mask_tokens' in name or 'output_upscaling' in name:
                param.data *= 1.1  # 10% í™•ì¥
        
        return decoder
    
    def _create_visible_decoder(self):
        """Visible ë§ˆìŠ¤í¬ì— íŠ¹í™”ëœ ë””ì½”ë”"""
        decoder = copy.deepcopy(self.efficient_sam.mask_decoder)
        
        # Visible íŠ¹í™”: ë” ì •í™•í•œ ê²½ê³„ë¥¼ ì˜ˆì¸¡í•˜ë„ë¡ ì´ˆê¸°í™”
        for name, param in decoder.named_parameters():
            if 'mask_tokens' in name or 'output_upscaling' in name:
                param.data *= 0.9  # 10% ì¶•ì†Œ
        
        return decoder

    def _freeze_sam_parameters(self):
        """SAMì˜ ëª¨ë“  íŒŒë¼ë¯¸í„°ë¥¼ ê³ ì •í•©ë‹ˆë‹¤ (Zero-shot ëª¨ë“œ)"""
        for param in self.image_encoder.parameters():
            param.requires_grad = False
        for param in self.prompt_encoder.parameters():
            param.requires_grad = False
        for param in self.amodal_mask_decoder.parameters():
            param.requires_grad = False
        for param in self.visible_mask_decoder.parameters():
            param.requires_grad = False
    
    def _initialize_d2sa_dataset(self, max_samples: int = 50):
        """D2SA ë°ì´í„°ì…‹ì„ ì´ˆê¸°í™”í•©ë‹ˆë‹¤."""
        print("ğŸ“Š D2SA ë°ì´í„°ì…‹ ì´ˆê¸°í™” ì¤‘...")
        
        # D2SA ê²½ë¡œ ì„¤ì •
        D2SA_ROOT = "/root/datasets/D2SA"
        TRAIN_ANNOTATION_FILE = os.path.join(D2SA_ROOT, "D2S_amodal_augmented.json")
        IMAGE_DIR = os.path.join(D2SA_ROOT, "images")
        
        # ë°ì´í„° ë³€í™˜ ì„¤ì •
        transform = transforms.Compose([
            transforms.Resize((1024, 1024)),
            transforms.ToTensor(),
        ])
        
        try:
            self.d2sa_dataset = D2SADataset(
                annotation_file=TRAIN_ANNOTATION_FILE,
                image_dir=IMAGE_DIR,
                transform=transform,
                max_samples=max_samples
            )
            
            self.d2sa_dataloader = DataLoader(
                self.d2sa_dataset, 
                batch_size=1, 
                shuffle=True, 
                num_workers=0
            )
            
            print(f"âœ“ D2SA ë°ì´í„°ì…‹ ë¡œë“œ ì™„ë£Œ: {len(self.d2sa_dataset)}ê°œ ìƒ˜í”Œ")
            
        except Exception as e:
            print(f"âŒ D2SA ë°ì´í„°ì…‹ ë¡œë“œ ì‹¤íŒ¨: {e}")
            print("ì‹¤ì œ D2SA ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
            self.use_d2sa = False
    
    def get_category_name(self, category_id: int) -> str:
        """COCO ì¹´í…Œê³ ë¦¬ IDë¥¼ ì¹´í…Œê³ ë¦¬ ì´ë¦„ìœ¼ë¡œ ë³€í™˜"""
        category_map = {
            1: "person", 2: "bicycle", 3: "car", 4: "motorcycle", 5: "airplane",
            6: "bus", 7: "train", 8: "truck", 9: "boat", 10: "traffic light",
            11: "fire hydrant", 13: "stop sign", 14: "parking meter", 15: "bench",
            16: "bird", 17: "cat", 18: "dog", 19: "horse", 20: "sheep",
            21: "cow", 22: "elephant", 23: "bear", 24: "zebra", 25: "giraffe",
            27: "backpack", 28: "umbrella", 31: "handbag", 32: "tie",
            33: "suitcase", 34: "frisbee", 35: "skis", 36: "snowboard",
            37: "sports ball", 38: "kite", 39: "baseball bat", 40: "baseball glove",
            41: "skateboard", 42: "surfboard", 43: "tennis racket", 44: "bottle",
            46: "wine glass", 47: "cup", 48: "fork", 49: "knife", 50: "spoon",
            51: "bowl", 52: "banana", 53: "apple", 54: "sandwich", 55: "orange",
            56: "broccoli", 57: "carrot", 58: "hot dog", 59: "pizza", 60: "donut",
            61: "cake", 62: "chair", 63: "couch", 64: "potted plant", 65: "bed",
            67: "dining table", 70: "toilet", 72: "tv", 73: "laptop", 74: "mouse",
            75: "remote", 76: "keyboard", 77: "cell phone", 78: "microwave",
            79: "oven", 80: "toaster", 81: "sink", 82: "refrigerator", 84: "book",
            85: "clock", 86: "vase", 87: "scissors", 88: "teddy bear", 89: "hair drier",
            90: "toothbrush"
        }
        
        return category_map.get(category_id, f"category_{category_id}")
    
    def get_d2sa_sample(self, index: int = None) -> Tuple[torch.Tensor, torch.Tensor, Image.Image, str, Dict]:
        """
        D2SA ë°ì´í„°ì…‹ì—ì„œ ìƒ˜í”Œì„ ê°€ì ¸ì˜µë‹ˆë‹¤.
        
        Args:
            index: íŠ¹ì • ì¸ë±ìŠ¤ (Noneì´ë©´ ëœë¤)
            
        Returns:
            Tuple: (image_tensor, bbox_tensor, pil_image, category_name, annotation_info)
        """
        if not self.use_d2sa or self.d2sa_dataset is None:
            raise RuntimeError("D2SA ë°ì´í„°ì…‹ì´ ì‚¬ìš© ë¶ˆê°€ëŠ¥í•©ë‹ˆë‹¤. ì‹¤ì œ ë°ì´í„°ê°€ í•„ìš”í•©ë‹ˆë‹¤.")
        
        try:
            if index is None:
                # ëœë¤ ìƒ˜í”Œ ì„ íƒ
                index = np.random.randint(0, len(self.d2sa_dataset))
            
            # ë°ì´í„° ë¡œë“œ (D2SADatasetì€ íŠœí”Œ ë°˜í™˜)
            data = self.d2sa_dataset[index]
            image, bbox, text, amodal_mask, visible_mask, invisible_mask, annotation_info = data
            
            # PIL ì´ë¯¸ì§€ ìƒì„±
            image_np = (image.permute(1, 2, 0).cpu().numpy() * 255).astype(np.uint8)
            pil_image = Image.fromarray(image_np)
            
            # ì¹´í…Œê³ ë¦¬ ì´ë¦„ ì¶”ì¶œ
            category_id = annotation_info['category_id'].item()
            category_name = self.get_category_name(category_id)
            
            # ì–´ë…¸í…Œì´ì…˜ ì •ë³´ ë”•ì…”ë„ˆë¦¬ë¡œ ë³€í™˜
            ann_dict = {
                'image_id': annotation_info['image_id'].item(),
                'category_id': category_id,
                'category_name': category_name,
                'file_name': annotation_info['file_name'],
                'area': annotation_info['area'].item(),
                'occlude_rate': annotation_info.get('occlude_rate', torch.tensor(0.0)).item()
            }
            
            print(f"ğŸ“¸ D2SA ìƒ˜í”Œ ë¡œë“œ ì™„ë£Œ:")
            print(f"  - ì¸ë±ìŠ¤: {index}")
            print(f"  - ì´ë¯¸ì§€ ID: {ann_dict['image_id']}")
            print(f"  - ì¹´í…Œê³ ë¦¬: {category_name} (ID: {category_id})")
            print(f"  - íŒŒì¼ëª…: {ann_dict['file_name']}")
            print(f"  - Occlusion rate: {ann_dict['occlude_rate']:.3f}")
            
            return image.unsqueeze(0), bbox.unsqueeze(0), pil_image, category_name, ann_dict
            
        except Exception as e:
            print(f"âŒ D2SA ìƒ˜í”Œ ë¡œë“œ ì‹¤íŒ¨ (index={index}): {e}")
            raise RuntimeError(f"D2SA ìƒ˜í”Œ ë¡œë“œ ì‹¤íŒ¨: {e}")
    

    def extract_occlusion_info(self, image_pil: Image.Image, bbox: np.ndarray = None, target_class: str = None, 
                             use_vlsam_method: bool = True, auto_detect_bbox: bool = True) -> Tuple[Dict, Dict, np.ndarray, np.ndarray, np.ndarray]:
        """
        VLMì„ ì‚¬ìš©í•˜ì—¬ ì´ë¯¸ì§€ì—ì„œ occlusion ì •ë³´ì™€ attention map, SAM promptë¥¼ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            image_pil (PIL.Image): ë¶„ì„í•  ì´ë¯¸ì§€
            bbox (np.ndarray, optional): ë°”ìš´ë”© ë°•ìŠ¤ [x1, y1, x2, y2]
            target_class (str, optional): íƒ€ê²Ÿ ê°ì²´ í´ë˜ìŠ¤
            use_vlsam_method (bool): VL-SAM ë…¼ë¬¸ ë°©ì‹ ì‚¬ìš© ì—¬ë¶€
            auto_detect_bbox (bool): bboxê°€ ì—†ì„ ë•Œ ìë™ íƒì§€ ìˆ˜í–‰ ì—¬ë¶€
            
        Returns:
            Tuple: (occlusion ê´€ê³„ ì •ë³´, layerë³„ attention maps, ì§‘ê³„ëœ attention map, sam_points, sam_labels)
        """
        print(f"ğŸ” VLMì„ ì‚¬ìš©í•œ Occlusion ê´€ê³„ ë¶„ì„ ì‹œì‘ ({'VL-SAM ë°©ì‹' if use_vlsam_method else 'ê¸°ë³¸ ë°©ì‹'})...")
        
        # 1. Occlusion ê´€ê³„ ë¶„ì„
        occlusion_info = self.occlusion_analyzer.analyze_occlusion(image_pil)
        
        print(f"âœ“ Occlusion ë¶„ì„ ì™„ë£Œ:")
        print(f"  - ë³´ì´ëŠ” ë¬¼ì²´: {occlusion_info.get('visible_objects', [])}")
        print(f"  - ê°€ë ¤ì§„ ë¬¼ì²´: {occlusion_info.get('occluded_objects', [])}")
        print(f"  - Occlusion ê´€ê³„: {occlusion_info.get('occlusion_relationships', [])}")
        
        # 2. Attention Map ì¶”ì¶œ (VL-SAM ë°©ì‹ ì„ íƒ ê°€ëŠ¥)
        attention_maps, aggregated_attention = self.occlusion_analyzer.extract_attention_maps(
            image_pil, use_vlsam_method=use_vlsam_method
        )
        
        # 3. SAM Prompt ìƒì„± (VL-SAM ë°©ì‹ ì„ íƒ ê°€ëŠ¥)
        sam_points, sam_labels = self.occlusion_analyzer.generate_sam_prompts(
            image_pil, attention_maps, aggregated_attention, bbox, target_class, use_vlsam_method=use_vlsam_method
        )
        
        return occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels

    def forward(self, image: torch.Tensor, box: torch.Tensor, image_pil: Optional[Image.Image] = None, text: str = None):
        """
        ë©”ì¸ forward í•¨ìˆ˜
        
        Args:
            image (torch.Tensor): ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ. Shape: (B, 3, H, W)
            box (torch.Tensor): ì…ë ¥ ë°”ìš´ë”© ë°•ìŠ¤. Shape: (B, 4) (x1, y1, x2, y2)
            image_pil (PIL.Image, optional): PIL ì´ë¯¸ì§€ (VLM ë¶„ì„ìš©)
            text (str, optional): ê°ì²´ í´ë˜ìŠ¤ í…ìŠ¤íŠ¸
            
        Returns:
            Tuple: (amodal_mask, amodal_iou, visible_mask, visible_iou, occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels)
        """
        batch_size, _, img_h, img_w = image.shape
        
        # 1. VLMì„ ì‚¬ìš©í•œ Occlusion ê´€ê³„ ë¶„ì„, Attention Map ì¶”ì¶œ, SAM Prompt ìƒì„± (ì²« ë²ˆì§¸ ì´ë¯¸ì§€ë§Œ ì‚¬ìš©)
        occlusion_info = None
        attention_maps = {}
        aggregated_attention = np.zeros((img_h, img_w))
        sam_points = np.array([[512, 512]])  # ê¸°ë³¸ê°’
        sam_labels = np.array([1])  # ê¸°ë³¸ê°’
        
        if image_pil is not None:
            # íƒ€ê²Ÿ í´ë˜ìŠ¤ ì¶”ì¶œ (í…ìŠ¤íŠ¸ì—ì„œ)
            target_class = None
            if text:
                # "a clementine" -> "clementine" í˜•íƒœë¡œ ë³€í™˜
                target_class = text.replace("a ", "").replace("an ", "").strip()
            
            occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels = self.extract_occlusion_info(
                image_pil, box[0].cpu().numpy(), target_class, use_vlsam_method=True
            )
        
        # 2. SAMì„ ì‚¬ìš©í•œ ë§ˆìŠ¤í¬ ì˜ˆì¸¡ (VLM-guided points ì‚¬ìš©)
        print("ğŸ¯ SAMì„ ì‚¬ìš©í•œ ë§ˆìŠ¤í¬ ì˜ˆì¸¡ ì‹œì‘...")
        print(f"  ğŸ“ VLM-guided points ì‚¬ìš©: {sam_points.shape}")
        
        # VLMì—ì„œ ì¶”ì¶œí•œ attention-based pointsë¥¼ SAM promptë¡œ ì‚¬ìš©
        if len(sam_points) > 0:
            # SAM pointsë¥¼ EfficientSAM í˜•íƒœë¡œ ë³€í™˜
            # sam_points: (N, 2), sam_labels: (N,)
            num_points = len(sam_points)
            
            # í¬ì¸íŠ¸ë“¤ì„ ë°°ì¹˜ í˜•íƒœë¡œ ì¬êµ¬ì„±: (B, Q, N, 2)
            points = torch.from_numpy(sam_points).float().to(image.device)
            points = points.unsqueeze(0).unsqueeze(0)  # (1, 1, N, 2)
            
            # ë¼ë²¨ë“¤ë„ ë™ì¼í•˜ê²Œ ë³€í™˜: (B, Q, N)
            labels = torch.from_numpy(sam_labels).long().to(image.device)
            labels = labels.unsqueeze(0).unsqueeze(0)  # (1, 1, N)
            
            print(f"  âœ“ VLM Points ë³€í™˜ ì™„ë£Œ: {num_points}ê°œ í¬ì¸íŠ¸")
            print(f"    - Positive: {torch.sum(labels).item()}ê°œ")
            print(f"    - Negative: {num_points - torch.sum(labels).item()}ê°œ")
            
        else:
            # Fallback: ë°”ìš´ë”© ë°•ìŠ¤ ì¤‘ì‹¬ì  ì‚¬ìš©
            print("  âš ï¸ VLM pointsê°€ ì—†ì–´ ë°”ìš´ë”© ë°•ìŠ¤ ì¤‘ì‹¬ì  ì‚¬ìš©")
            center_x = (box[:, 0] + box[:, 2]) / 2
            center_y = (box[:, 1] + box[:, 3]) / 2
            
            points = torch.stack([center_x, center_y], dim=1).unsqueeze(1).unsqueeze(1)  # (B, 1, 1, 2)
            labels = torch.ones(batch_size, 1, 1, device=image.device, dtype=torch.int)

        try:
            # ê³µí†µ ì´ë¯¸ì§€ ë° í”„ë¡¬í”„íŠ¸ ì„ë² ë”©
            with torch.no_grad():
                image_embeddings = self.image_encoder(image)
            image_embeddings = image_embeddings.detach()
            
            # í”„ë¡¬í”„íŠ¸ ì„ë² ë”© (VLM-guided points ì²˜ë¦¬)
            if len(sam_points) > 0:
                # ë‹¤ì¤‘ í¬ì¸íŠ¸ ì²˜ë¦¬
                num_points = points.shape[2]  # Nê°œ í¬ì¸íŠ¸
                
                # Points rescaling
                rescaled_points = self.efficient_sam.get_rescaled_pts(points, img_h, img_w)
                
                # Sparse embeddings ìƒì„±
                sparse_embeddings = self.prompt_encoder(
                    rescaled_points.reshape(batch_size, num_points, 2),
                    labels.reshape(batch_size, num_points),
                )
                
                print(f"  âœ“ Sparse embeddings ìƒì„±: {sparse_embeddings.shape}")
                
            else:
                # Fallback: ë‹¨ì¼ í¬ì¸íŠ¸ ì²˜ë¦¬
                rescaled_points = self.efficient_sam.get_rescaled_pts(points, img_h, img_w)
                sparse_embeddings = self.prompt_encoder(
                    rescaled_points.reshape(batch_size, 1, 2),
                    labels.reshape(batch_size, 1),
                )
            
            # Sparse embeddings shape ì¡°ì •
            if len(sparse_embeddings.shape) == 3:
                sparse_embeddings = sparse_embeddings.unsqueeze(1)  # (B, 1, embedding_dim, embedding_size)
            
            # Amodal ë§ˆìŠ¤í¬ ì˜ˆì¸¡
            amodal_logits, amodal_iou = self.amodal_mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=self.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                multimask_output=True,
            )
            
            # Visible ë§ˆìŠ¤í¬ ì˜ˆì¸¡
            visible_logits, visible_iou = self.visible_mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=self.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                multimask_output=True,
            )
            
            # ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬
            if len(amodal_logits.shape) == 5:  # (B, Q, N, H, W)
                amodal_mask = amodal_logits[:, 0, 0:1, :, :]
                amodal_iou_best = amodal_iou[:, 0, 0:1]
                visible_mask = visible_logits[:, 0, 0:1, :, :]
                visible_iou_best = visible_iou[:, 0, 0:1]
            elif len(amodal_logits.shape) == 4:  # (B, N, H, W)
                amodal_mask = amodal_logits[:, 0:1, :, :]
                amodal_iou_best = amodal_iou[:, 0:1]
                visible_mask = visible_logits[:, 0:1, :, :]
                visible_iou_best = visible_iou[:, 0:1]
            else:
                raise ValueError(f"Unexpected logits shape: amodal={amodal_logits.shape}, visible={visible_logits.shape}")
            
            # ë§ˆìŠ¤í¬ë¥¼ ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
            if amodal_mask.shape[-2:] != (img_h, img_w):
                amodal_mask = F.interpolate(
                    amodal_mask, size=(img_h, img_w), mode='bilinear', align_corners=False
                )
            if visible_mask.shape[-2:] != (img_h, img_w):
                visible_mask = F.interpolate(
                    visible_mask, size=(img_h, img_w), mode='bilinear', align_corners=False
                )

            print("âœ“ ë§ˆìŠ¤í¬ ì˜ˆì¸¡ ì™„ë£Œ")
            
            return amodal_mask, amodal_iou_best, visible_mask, visible_iou_best, occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels

        except Exception as e:
            print(f"ëª¨ë¸ ì¶”ë¡  ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
            import traceback
            traceback.print_exc()
            # ì˜¤ë¥˜ ë°œìƒ ì‹œ ë¹ˆ ë§ˆìŠ¤í¬ ë°˜í™˜
            empty_mask = torch.zeros(batch_size, 1, img_h, img_w, device=image.device)
            empty_iou = torch.zeros(batch_size, 1, device=image.device)
            return empty_mask, empty_iou, empty_mask.clone(), empty_iou.clone(), occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels

    def create_pipeline_visualization(self, 
                                    image_pil: Image.Image,
                                    results: Tuple,
                                    bbox: np.ndarray = None,
                                    gt_amodal: Optional[np.ndarray] = None,
                                    gt_visible: Optional[np.ndarray] = None,
                                    save_path: str = None,
                                    title: str = "VLM-SAM Pipeline") -> None:
        """
        ì „ì²´ íŒŒì´í”„ë¼ì¸ì˜ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            image_pil: ì›ë³¸ PIL ì´ë¯¸ì§€
            results: forward í•¨ìˆ˜ì˜ ë°˜í™˜ê°’
            bbox: ë°”ìš´ë”© ë°•ìŠ¤
            gt_amodal: GT amodal mask (optional)
            gt_visible: GT visible mask (optional)
            save_path: ì €ì¥ ê²½ë¡œ
            title: ì œëª©
        """
        try:
            # Results unpacking
            pred_amodal, pred_amodal_iou, pred_visible, pred_visible_iou, occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels = results
            
            # PIL ì´ë¯¸ì§€ë¥¼ numpy arrayë¡œ ë³€í™˜
            image_np = np.array(image_pil)
            
            # ì˜ˆì¸¡ ë§ˆìŠ¤í¬ë¥¼ numpyë¡œ ë³€í™˜
            pred_amodal_np = torch.sigmoid(pred_amodal[0]).squeeze().cpu().numpy()
            pred_visible_np = torch.sigmoid(pred_visible[0]).squeeze().cpu().numpy()
            
            print(f"ğŸ¨ íŒŒì´í”„ë¼ì¸ ì‹œê°í™” ìƒì„± ì¤‘...")
            print(f"  - ì´ë¯¸ì§€ í¬ê¸°: {image_np.shape}")
            print(f"  - Attention maps: {len(attention_maps)}ê°œ")
            print(f"  - SAM points: {len(sam_points)}ê°œ")
            print(f"  - ì˜ˆì¸¡ ë§ˆìŠ¤í¬: Amodal {pred_amodal_np.shape}, Visible {pred_visible_np.shape}")
            
            # í†µí•© ì‹œê°í™” ìƒì„±
            self.visualizer.visualize_complete_pipeline(
                image=image_np,
                attention_maps=attention_maps,
                aggregated_attention=aggregated_attention,
                sam_points=sam_points,
                sam_labels=sam_labels,
                pred_amodal_mask=pred_amodal_np,
                pred_visible_mask=pred_visible_np,
                gt_amodal_mask=gt_amodal,
                gt_visible_mask=gt_visible,
                bbox=bbox,
                save_path=save_path,
                title=title
            )
            
            print(f"âœ“ íŒŒì´í”„ë¼ì¸ ì‹œê°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ íŒŒì´í”„ë¼ì¸ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def create_attention_analysis(self,
                                attention_maps: Dict,
                                aggregated_attention: np.ndarray,
                                save_path: str = None) -> None:
        """
        Attention Map ë¶„ì„ ì‹œê°í™”ë¥¼ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            attention_maps: Layerë³„ attention maps
            aggregated_attention: ì§‘ê³„ëœ attention map
            save_path: ì €ì¥ ê²½ë¡œ
        """
        try:
            self.visualizer.create_attention_analysis(
                attention_maps=attention_maps,
                aggregated_attention=aggregated_attention,
                save_path=save_path
            )
            print(f"âœ“ Attention ë¶„ì„ ì‹œê°í™” ì™„ë£Œ")
            
        except Exception as e:
            print(f"âŒ Attention ë¶„ì„ ì‹œê°í™” ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
    
    def process_d2sa_sample(self, index: int = None, save_visualization: bool = True, save_vlm_analysis: bool = True, output_dir: str = "./outputs/d2sa_vlm_sam_250909") -> Dict:
        """
        D2SA ìƒ˜í”Œì„ ì²˜ë¦¬í•˜ê³  ê²°ê³¼ë¥¼ ë°˜í™˜í•©ë‹ˆë‹¤.
        
        Args:
            index: ì²˜ë¦¬í•  ìƒ˜í”Œ ì¸ë±ìŠ¤ (Noneì´ë©´ ëœë¤)
            save_visualization: ì‹œê°í™” ì €ì¥ ì—¬ë¶€
            save_vlm_analysis: VLM ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì—¬ë¶€
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼
        """
        print(f"ğŸš€ D2SA ìƒ˜í”Œ ì²˜ë¦¬ ì‹œì‘ (index={index})")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        if save_visualization or save_vlm_analysis:
            os.makedirs(output_dir, exist_ok=True)
        
        try:
            # D2SA ìƒ˜í”Œ ë¡œë“œ
            image_tensor, bbox_tensor, pil_image, category_name, ann_info = self.get_d2sa_sample(index)
            
            # VLM-SAM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            print(f"ğŸ”„ VLM-SAM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì¤‘...")
            
            with torch.no_grad():
                results = self.forward(
                    image=image_tensor,
                    box=bbox_tensor,
                    image_pil=pil_image,
                    text=category_name
                )
            
            # ê²°ê³¼ ì–¸íŒ¨í‚¹
            pred_amodal, pred_amodal_iou, pred_visible, pred_visible_iou, occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels = results
            
            print(f"âœ… íŒŒì´í”„ë¼ì¸ ì‹¤í–‰ ì™„ë£Œ!")
            print(f"  - Pred Amodal IoU: {pred_amodal_iou[0].item():.3f}")
            print(f"  - Pred Visible IoU: {pred_visible_iou[0].item():.3f}")
            print(f"  - VLM Points: {len(sam_points)}ê°œ")
            print(f"  - Attention Layers: {len(attention_maps)}ê°œ")
            
            # ì‹œê°í™” ì €ì¥
            viz_path = None
            if save_visualization:
                viz_filename = f"d2sa_{ann_info['image_id']}_{category_name}_{index or 'random'}.png"
                viz_path = os.path.join(output_dir, viz_filename)
                
                self.create_pipeline_visualization(
                    image_pil=pil_image,
                    results=results,
                    bbox=bbox_tensor[0].cpu().numpy(),
                    save_path=viz_path,
                    title=f"D2SA Sample - {category_name} (ID: {ann_info['image_id']})"
                )
                
                print(f"ğŸ¨ ì‹œê°í™” ì €ì¥: {viz_path}")
            
            # VLM ë¶„ì„ ê²°ê³¼ JSON ì €ì¥
            vlm_json_path = None
            if save_vlm_analysis and occlusion_info:
                vlm_filename = f"vlm_analysis_{ann_info['image_id']}_{category_name}_{index or 'random'}.json"
                vlm_json_path = os.path.join(output_dir, vlm_filename)
                
                # VLM ë¶„ì„ ê²°ê³¼ ì •ë¦¬
                vlm_analysis_data = {
                    "image_info": {
                        "image_id": ann_info['image_id'],
                        "file_name": ann_info['file_name'],
                        "category_name": category_name,
                        "category_id": ann_info['category_id'],
                        "occlusion_rate": ann_info.get('occlude_rate', 0.0),
                        "sample_index": index
                    },
                    "vlm_analysis": {
                        "visible_objects": occlusion_info.get('visible_objects', []),
                        "occluded_objects": occlusion_info.get('occluded_objects', []),
                        "occlusion_relationships": occlusion_info.get('occlusion_relationships', [])
                    },
                    "analysis_stats": {
                        "num_visible_objects": len(occlusion_info.get('visible_objects', [])),
                        "num_occluded_objects": len(occlusion_info.get('occluded_objects', [])),
                        "num_occlusion_relationships": len(occlusion_info.get('occlusion_relationships', [])),
                        "all_detected_objects": list(set(
                            occlusion_info.get('visible_objects', []) + 
                            occlusion_info.get('occluded_objects', [])
                        )),
                        "ground_truth_object": category_name
                    },
                    "processing_info": {
                        "pred_amodal_iou": float(pred_amodal_iou[0].item()),
                        "pred_visible_iou": float(pred_visible_iou[0].item()),
                        "num_sam_points": len(sam_points),
                        "num_attention_layers": len(attention_maps)
                    }
                }
                
                # JSON íŒŒì¼ë¡œ ì €ì¥
                with open(vlm_json_path, 'w', encoding='utf-8') as f:
                    json.dump(vlm_analysis_data, f, indent=2, ensure_ascii=False)
                
                print(f"ğŸ“„ VLM ë¶„ì„ ê²°ê³¼ ì €ì¥: {vlm_json_path}")
            
            # ê²°ê³¼ ì •ë¦¬
            result = {
                "sample_info": ann_info,
                "processing_results": {
                    "pred_amodal_iou": float(pred_amodal_iou[0].item()),
                    "pred_visible_iou": float(pred_visible_iou[0].item()),
                    "num_sam_points": len(sam_points),
                    "num_positive_points": int(np.sum(sam_labels)) if len(sam_labels) > 0 else 0,
                    "num_negative_points": int(len(sam_points) - np.sum(sam_labels)) if len(sam_labels) > 0 else 0,
                    "num_attention_layers": len(attention_maps),
                    "attention_stats": {
                        "mean": float(np.mean(aggregated_attention)),
                        "std": float(np.std(aggregated_attention)),
                        "max": float(np.max(aggregated_attention)),
                        "min": float(np.min(aggregated_attention))
                    }
                },
                "vlm_analysis": occlusion_info,
                "visualization_path": viz_path if save_visualization else None,
                "vlm_json_path": vlm_json_path if save_vlm_analysis else None,
                "status": "success"
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ D2SA ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "sample_info": {"error": str(e)},
                "status": "error",
                "error_details": traceback.format_exc()
            }
    
    def process_multiple_d2sa_samples(self, num_samples: int = 5, save_vlm_analysis: bool = True, output_dir: str = "./outputs/d2sa_vlm_sam_250909") -> List[Dict]:
        """
        ì—¬ëŸ¬ D2SA ìƒ˜í”Œì„ ë°°ì¹˜ ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            num_samples: ì²˜ë¦¬í•  ìƒ˜í”Œ ìˆ˜
            save_vlm_analysis: VLM ë¶„ì„ ê²°ê³¼ JSON ì €ì¥ ì—¬ë¶€
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            List[Dict]: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
        """
        print(f"ğŸš€ D2SA ë°°ì¹˜ ì²˜ë¦¬ ì‹œì‘: {num_samples}ê°œ ìƒ˜í”Œ")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        os.makedirs(output_dir, exist_ok=True)
        
        results = []
        successful = 0
        failed = 0
        
        for i in range(num_samples):
            print(f"\n--- ìƒ˜í”Œ {i+1}/{num_samples} ì²˜ë¦¬ ì¤‘ ---")
            
            try:
                result = self.process_d2sa_sample(
                    index=None,  # ëœë¤ ì„ íƒ
                    save_visualization=True,
                    save_vlm_analysis=save_vlm_analysis,
                    output_dir=output_dir
                )
                
                results.append(result)
                
                if result["status"] == "success":
                    successful += 1
                else:
                    failed += 1
                    
            except Exception as e:
                print(f"âŒ ìƒ˜í”Œ {i+1} ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                failed += 1
                
                error_result = {
                    "sample_index": i,
                    "status": "error",
                    "error": str(e)
                }
                results.append(error_result)
        
        print(f"\nğŸ“Š ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
        print(f"  - ì„±ê³µ: {successful}ê°œ")
        print(f"  - ì‹¤íŒ¨: {failed}ê°œ")
        print(f"  - ì„±ê³µë¥ : {successful/num_samples*100:.1f}%")
        
        # VLM ë¶„ì„ í†µí•© ìš”ì•½ ìƒì„±
        vlm_summary = None
        if save_vlm_analysis:
            vlm_summary = self._create_vlm_analysis_summary(results, output_dir)
        
        # ê²°ê³¼ ìš”ì•½ ì €ì¥
        summary = {
            "total_samples": num_samples,
            "successful": successful,
            "failed": failed,
            "success_rate": successful/num_samples*100,
            "output_directory": output_dir,
            "vlm_analysis_enabled": save_vlm_analysis,
            "vlm_summary_path": vlm_summary.get("summary_path") if vlm_summary else None,
            "results": results
        }
        
        summary_path = os.path.join(output_dir, "batch_processing_summary.json")
        with open(summary_path, 'w') as f:
            json.dump(summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ ë°°ì¹˜ ì²˜ë¦¬ ìš”ì•½ ì €ì¥: {summary_path}")
        
        return results
    
    def _create_vlm_analysis_summary(self, results: List[Dict], output_dir: str) -> Dict:
        """
        VLM ë¶„ì„ ê²°ê³¼ë“¤ì˜ í†µí•© ìš”ì•½ì„ ìƒì„±í•©ë‹ˆë‹¤.
        
        Args:
            results: ì²˜ë¦¬ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict: VLM ë¶„ì„ ìš”ì•½ ì •ë³´
        """
        print(f"ğŸ“Š VLM ë¶„ì„ í†µí•© ìš”ì•½ ìƒì„± ì¤‘...")
        
        successful_results = [r for r in results if r.get("status") == "success" and r.get("vlm_analysis")]
        
        if not successful_results:
            print("âš ï¸ ì„±ê³µì ì¸ VLM ë¶„ì„ ê²°ê³¼ê°€ ì—†ì–´ ìš”ì•½ì„ ìƒì„±í•  ìˆ˜ ì—†ìŠµë‹ˆë‹¤.")
            return {"summary_path": None}
        
        # ëª¨ë“  ê°ì§€ëœ ê°ì²´ í´ë˜ìŠ¤ ìˆ˜ì§‘
        all_visible_objects = []
        all_occluded_objects = []
        all_detected_objects = []
        object_frequency = {}
        category_analysis = {}
        
        for result in successful_results:
            vlm_data = result["vlm_analysis"]
            sample_info = result["sample_info"]
            
            # ê° ì¹´í…Œê³ ë¦¬ë³„ ë¶„ì„
            category_name = sample_info["category_name"]
            if category_name not in category_analysis:
                category_analysis[category_name] = {
                    "count": 0,
                    "visible_objects": [],
                    "occluded_objects": [],
                    "avg_visible_count": 0,
                    "avg_occluded_count": 0,
                    "ground_truth_detected": 0
                }
            
            category_info = category_analysis[category_name]
            category_info["count"] += 1
            
            # ê°€ì‹œì  ê°ì²´
            visible_objs = vlm_data.get("visible_objects", [])
            all_visible_objects.extend(visible_objs)
            category_info["visible_objects"].extend(visible_objs)
            
            # ê°€ë ¤ì§„ ê°ì²´
            occluded_objs = vlm_data.get("occluded_objects", [])
            all_occluded_objects.extend(occluded_objs)
            category_info["occluded_objects"].extend(occluded_objs)
            
            # ëª¨ë“  ê°ì§€ëœ ê°ì²´
            detected_objs = visible_objs + occluded_objs
            all_detected_objects.extend(detected_objs)
            
            # Ground Truth ê°ì²´ê°€ ê°ì§€ë˜ì—ˆëŠ”ì§€ í™•ì¸
            if category_name.lower() in [obj.lower() for obj in detected_objs]:
                category_info["ground_truth_detected"] += 1
            
            # ê°ì²´ ë¹ˆë„ ê³„ì‚°
            for obj in detected_objs:
                obj_lower = obj.lower()
                object_frequency[obj_lower] = object_frequency.get(obj_lower, 0) + 1
        
        # ì¹´í…Œê³ ë¦¬ë³„ í†µê³„ ê³„ì‚°
        for category_name, info in category_analysis.items():
            if info["count"] > 0:
                info["avg_visible_count"] = len(info["visible_objects"]) / info["count"]
                info["avg_occluded_count"] = len(info["occluded_objects"]) / info["count"]
                info["ground_truth_detection_rate"] = info["ground_truth_detected"] / info["count"] * 100
                # ì¤‘ë³µ ì œê±°
                info["unique_visible_objects"] = list(set(info["visible_objects"]))
                info["unique_occluded_objects"] = list(set(info["occluded_objects"]))
        
        # ì „ì²´ í†µê³„
        total_stats = {
            "total_samples_analyzed": len(successful_results),
            "total_unique_objects_detected": len(set(all_detected_objects)),
            "total_unique_visible_objects": len(set(all_visible_objects)),
            "total_unique_occluded_objects": len(set(all_occluded_objects)),
            "most_frequent_objects": sorted(object_frequency.items(), key=lambda x: x[1], reverse=True)[:10],
            "avg_objects_per_image": len(all_detected_objects) / len(successful_results) if successful_results else 0,
            "avg_visible_per_image": len(all_visible_objects) / len(successful_results) if successful_results else 0,
            "avg_occluded_per_image": len(all_occluded_objects) / len(successful_results) if successful_results else 0
        }
        
        # VLM ë¶„ì„ ìš”ì•½ ë°ì´í„°
        vlm_analysis_summary = {
            "summary_info": {
                "analysis_date": json.dumps({"timestamp": "generated"}).replace('"generated"', f'"{str(np.datetime64("now"))[:-10]}"'),
                "total_samples": len(successful_results),
                "analysis_method": "LLaVA-1.5-7b VLM"
            },
            "overall_statistics": total_stats,
            "category_analysis": category_analysis,
            "detailed_results": [
                {
                    "image_id": r["sample_info"]["image_id"],
                    "file_name": r["sample_info"]["file_name"],
                    "ground_truth_category": r["sample_info"]["category_name"],
                    "detected_visible": r["vlm_analysis"].get("visible_objects", []),
                    "detected_occluded": r["vlm_analysis"].get("occluded_objects", []),
                    "occlusion_relationships": r["vlm_analysis"].get("occlusion_relationships", []),
                    "processing_success": True
                }
                for r in successful_results
            ]
        }
        
        # VLM ë¶„ì„ ìš”ì•½ JSON ì €ì¥
        vlm_summary_path = os.path.join(output_dir, "vlm_analysis_summary.json")
        with open(vlm_summary_path, 'w', encoding='utf-8') as f:
            json.dump(vlm_analysis_summary, f, indent=2, ensure_ascii=False)
        
        print(f"ğŸ“„ VLM ë¶„ì„ í†µí•© ìš”ì•½ ì €ì¥: {vlm_summary_path}")
        print(f"ğŸ“Š ìš”ì•½ í†µê³„:")
        print(f"  - ë¶„ì„ëœ ìƒ˜í”Œ: {total_stats['total_samples_analyzed']}ê°œ")
        print(f"  - ê°ì§€ëœ ê³ ìœ  ê°ì²´: {total_stats['total_unique_objects_detected']}ê°œ")
        print(f"  - ì´ë¯¸ì§€ë‹¹ í‰ê·  ê°ì²´: {total_stats['avg_objects_per_image']:.1f}ê°œ")
        print(f"  - ê°€ì¥ ë¹ˆë²ˆí•œ ê°ì²´: {total_stats['most_frequent_objects'][:3]}")
        
        return {
            "summary_path": vlm_summary_path,
            "statistics": total_stats,
            "category_analysis": category_analysis
        }
    
    def iterative_refinement(self, image: torch.Tensor, initial_points: np.ndarray, 
                           initial_labels: np.ndarray, attention_map: np.ndarray,
                           num_iterations: int = 2) -> Tuple[torch.Tensor, torch.Tensor, List[Dict]]:
        """
        3.4 Iterative Refinement êµ¬í˜„
        PerSAM ë°©ì‹ì˜ cascaded post-refinementì™€ ë§ˆìŠ¤í¬ëœ attention mapì„ ì´ìš©í•œ ë°˜ë³µì  ê°œì„ 
        
        Args:
            image: ì…ë ¥ ì´ë¯¸ì§€ í…ì„œ
            initial_points: ì´ˆê¸° SAM í¬ì¸íŠ¸
            initial_labels: ì´ˆê¸° SAM ë¼ë²¨
            attention_map: ì§‘ê³„ëœ attention map
            num_iterations: ë°˜ë³µ íšŸìˆ˜
            
        Returns:
            Tuple: (ìµœì¢… amodal mask, ìµœì¢… visible mask, ë°˜ë³µ ê²°ê³¼ ë¦¬ìŠ¤íŠ¸)
        """
        print(f"ğŸ”„ Iterative Refinement ì‹œì‘: {num_iterations}íšŒ ë°˜ë³µ")
        
        batch_size, _, img_h, img_w = image.shape
        current_points = initial_points.copy()
        current_labels = initial_labels.copy()
        refinement_results = []
        
        # ì´ˆê¸° ë§ˆìŠ¤í¬ ìƒì„±
        amodal_masks = []
        visible_masks = []
        
        for iteration in range(num_iterations + 1):  # 0: ì´ˆê¸°, 1~N: ë°˜ë³µ
            print(f"  ğŸ“ ë°˜ë³µ {iteration}/{num_iterations}")
            
            # SAM ë§ˆìŠ¤í¬ ì˜ˆì¸¡
            amodal_mask, visible_mask = self._predict_sam_masks(
                image, current_points, current_labels
            )
            
            amodal_masks.append(amodal_mask)
            visible_masks.append(visible_mask)
            
            # ë°˜ë³µ ê²°ê³¼ ì €ì¥
            iteration_result = {
                "iteration": iteration,
                "points": current_points.copy(),
                "labels": current_labels.copy(),
                "amodal_iou": self._calculate_mask_iou(amodal_mask, None),  # GT ì—†ì´ ê¸°ë³¸ê°’
                "visible_iou": self._calculate_mask_iou(visible_mask, None)
            }
            refinement_results.append(iteration_result)
            
            if iteration < num_iterations:
                # ë‹¤ìŒ ë°˜ë³µì„ ìœ„í•œ í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸
                if iteration == 0:
                    # ì²« ë²ˆì§¸ ë°˜ë³µ: PerSAM ë°©ì‹ - ë§ˆìŠ¤í¬ë¥¼ ì¶”ê°€ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©
                    current_points, current_labels = self._per_sam_refinement(
                        current_points, current_labels, amodal_mask, visible_mask
                    )
                else:
                    # í›„ì† ë°˜ë³µ: ë§ˆìŠ¤í¬ëœ attention mapì—ì„œ ìƒˆë¡œìš´ í¬ì¸íŠ¸ ìƒì„±
                    masked_attention = self._mask_attention_map(attention_map, amodal_mask)
                    new_points, new_labels = self._generate_points_from_masked_attention(
                        masked_attention, current_points, current_labels
                    )
                    current_points = new_points
                    current_labels = new_labels
                
                print(f"    âœ“ í¬ì¸íŠ¸ ì—…ë°ì´íŠ¸: {len(current_points)}ê°œ")
        
        # ìµœì¢… ë§ˆìŠ¤í¬ ì„ íƒ (ë§ˆì§€ë§‰ ë°˜ë³µ ê²°ê³¼)
        final_amodal = amodal_masks[-1]
        final_visible = visible_masks[-1]
        
        print(f"âœ… Iterative Refinement ì™„ë£Œ: {len(refinement_results)}ê°œ ë°˜ë³µ")
        return final_amodal, final_visible, refinement_results
    
    def multi_scale_ensemble(self, image_pil: Image.Image, target_class: str = None,
                           num_iterations: int = 2) -> Tuple[torch.Tensor, torch.Tensor, Dict]:
        """
        3.5 Multi-scale Ensemble êµ¬í˜„
        ì´ë¯¸ì§€ë¥¼ 4ê°œ ì„œë¸Œì´ë¯¸ì§€ë¡œ ë¶„í• í•˜ì—¬ ì²˜ë¦¬í•˜ê³  NMSë¡œ ì§‘ê³„
        
        Args:
            image_pil: ì…ë ¥ PIL ì´ë¯¸ì§€
            target_class: íƒ€ê²Ÿ ê°ì²´ í´ë˜ìŠ¤
            num_iterations: ê° ì„œë¸Œì´ë¯¸ì§€ì—ì„œì˜ ë°˜ë³µ íšŸìˆ˜
            
        Returns:
            Tuple: (ì§‘ê³„ëœ amodal mask, ì§‘ê³„ëœ visible mask, ì„œë¸Œì´ë¯¸ì§€ ê²°ê³¼ë“¤)
        """
        print(f"ğŸ” Multi-scale Ensemble ì‹œì‘: 4ê°œ ì„œë¸Œì´ë¯¸ì§€ ì²˜ë¦¬")
        
        w, h = image_pil.size
        sub_image_size = (w // 2, h // 2)
        
        # 4ê°œ ì„œë¸Œì´ë¯¸ì§€ ìƒì„± (4ê°œ ëª¨ì„œë¦¬ì—ì„œ)
        sub_images = self._create_sub_images(image_pil, sub_image_size)
        
        sub_results = []
        all_amodal_masks = []
        all_visible_masks = []
        
        for i, (sub_image, offset) in enumerate(sub_images):
            print(f"  ğŸ“¸ ì„œë¸Œì´ë¯¸ì§€ {i+1}/4 ì²˜ë¦¬ ì¤‘...")
            
            # ì„œë¸Œì´ë¯¸ì§€ì— ëŒ€í•œ VLM-SAM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
            sub_result = self._process_sub_image(
                sub_image, target_class, offset, num_iterations
            )
            
            sub_results.append(sub_result)
            
            if sub_result["status"] == "success":
                all_amodal_masks.append(sub_result["amodal_mask"])
                all_visible_masks.append(sub_result["visible_mask"])
        
        # NMSë¡œ ê²°ê³¼ ì§‘ê³„
        if len(all_amodal_masks) > 0:
            print(f"  ğŸ”„ NMS ì§‘ê³„ ì¤‘... ({len(all_amodal_masks)}ê°œ ì„œë¸Œì´ë¯¸ì§€ ê²°ê³¼)")
            
            # ì„œë¸Œì´ë¯¸ì§€ ë§ˆìŠ¤í¬ë“¤ì„ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›í•˜ê³  ì§‘ê³„
            final_amodal, final_visible = self._aggregate_with_nms(
                all_amodal_masks, all_visible_masks, sub_results, (w, h)
            )
            
            print(f"âœ… Multi-scale Ensemble ì™„ë£Œ")
            return final_amodal, final_visible, {"sub_results": sub_results}
        else:
            print(f"âŒ ëª¨ë“  ì„œë¸Œì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨")
            # Fallback: ì›ë³¸ ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬
            return self._fallback_original_image_processing(image_pil, target_class)
    
    def _create_sub_images(self, image_pil: Image.Image, sub_size: tuple) -> List[Tuple[Image.Image, tuple]]:
        """4ê°œ ì„œë¸Œì´ë¯¸ì§€ ìƒì„± (4ê°œ ëª¨ì„œë¦¬ì—ì„œ)"""
        w, h = image_pil.size
        sub_w, sub_h = sub_size
        
        sub_images = []
        
        # 4ê°œ ëª¨ì„œë¦¬ ìœ„ì¹˜
        positions = [
            (0, 0),  # ì¢Œìƒë‹¨
            (w - sub_w, 0),  # ìš°ìƒë‹¨
            (0, h - sub_h),  # ì¢Œí•˜ë‹¨
            (w - sub_w, h - sub_h)  # ìš°í•˜ë‹¨
        ]
        
        for x, y in positions:
            # ì„œë¸Œì´ë¯¸ì§€ í¬ê¸° ì¡°ì • (ì´ë¯¸ì§€ ê²½ê³„ ë‚´ë¡œ)
            actual_w = min(sub_w, w - x)
            actual_h = min(sub_h, h - y)
            
            if actual_w > 0 and actual_h > 0:
                sub_image = image_pil.crop((x, y, x + actual_w, y + actual_h))
                # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
                sub_image = sub_image.resize((sub_w, sub_h), Image.Resampling.LANCZOS)
                sub_images.append((sub_image, (x, y)))
        
        return sub_images
    
    def _process_sub_image(self, sub_image: Image.Image, target_class: str, 
                          offset: tuple, num_iterations: int) -> Dict:
        """ê°œë³„ ì„œë¸Œì´ë¯¸ì§€ ì²˜ë¦¬"""
        try:
            # ì„œë¸Œì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            transform = transforms.Compose([
                transforms.Resize((1024, 1024)),
                transforms.ToTensor(),
            ])
            
            image_tensor = transform(sub_image).unsqueeze(0)
            
            # VLM ë¶„ì„ ë° attention map ì¶”ì¶œ
            occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels = self.extract_occlusion_info(
                sub_image, bbox=None, target_class=target_class, use_vlsam_method=True, auto_detect_bbox=True
            )
            
            if len(sam_points) == 0:
                return {"status": "error", "error": "No SAM points generated"}
            
            # Iterative Refinement ì ìš©
            final_amodal, final_visible, refinement_results = self.iterative_refinement(
                image_tensor, sam_points, sam_labels, aggregated_attention, num_iterations
            )
            
            return {
                "status": "success",
                "amodal_mask": final_amodal,
                "visible_mask": final_visible,
                "refinement_results": refinement_results,
                "offset": offset,
                "occlusion_info": occlusion_info
            }
            
        except Exception as e:
            return {"status": "error", "error": str(e)}
    
    def _aggregate_with_nms(self, amodal_masks: List[torch.Tensor], visible_masks: List[torch.Tensor],
                           sub_results: List[Dict], original_size: tuple) -> Tuple[torch.Tensor, torch.Tensor]:
        """NMSë¥¼ ì‚¬ìš©í•˜ì—¬ ì„œë¸Œì´ë¯¸ì§€ ê²°ê³¼ë“¤ì„ ì§‘ê³„"""
        w, h = original_size
        
        # ê° ì„œë¸Œì´ë¯¸ì§€ ë§ˆìŠ¤í¬ë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
        restored_amodal_masks = []
        restored_visible_masks = []
        
        for i, (amodal_mask, visible_mask, sub_result) in enumerate(zip(amodal_masks, visible_masks, sub_results)):
            if sub_result["status"] != "success":
                continue
                
            offset_x, offset_y = sub_result["offset"]
            
            # ì›ë³¸ í¬ê¸°ë¡œ ë³µì›
            restored_amodal = self._restore_mask_to_original_size(
                amodal_mask, (w, h), (offset_x, offset_y)
            )
            restored_visible = self._restore_mask_to_original_size(
                visible_mask, (w, h), (offset_x, offset_y)
            )
            
            restored_amodal_masks.append(restored_amodal)
            restored_visible_masks.append(restored_visible)
        
        if len(restored_amodal_masks) == 0:
            # Fallback: ë¹ˆ ë§ˆìŠ¤í¬ ë°˜í™˜
            empty_amodal = torch.zeros(1, 1, h, w)
            empty_visible = torch.zeros(1, 1, h, w)
            return empty_amodal, empty_visible
        
        # ë§ˆìŠ¤í¬ë“¤ì„ í‰ê· ìœ¼ë¡œ ì§‘ê³„ (ê°„ë‹¨í•œ NMS ëŒ€ì•ˆ)
        final_amodal = torch.stack(restored_amodal_masks).mean(dim=0)
        final_visible = torch.stack(restored_visible_masks).mean(dim=0)
        
        # ì„ê³„ê°’ ì ìš©
        final_amodal = (torch.sigmoid(final_amodal) > 0.5).float()
        final_visible = (torch.sigmoid(final_visible) > 0.5).float()
        
        return final_amodal, final_visible
    
    def _restore_mask_to_original_size(self, mask: torch.Tensor, original_size: tuple, 
                                     offset: tuple) -> torch.Tensor:
        """ë§ˆìŠ¤í¬ë¥¼ ì›ë³¸ í¬ê¸°ë¡œ ë³µì›"""
        w, h = original_size
        offset_x, offset_y = offset
        
        # ì›ë³¸ í¬ê¸°ë¡œ ë¦¬ì‚¬ì´ì¦ˆ
        restored = F.interpolate(
            mask, size=(h, w), mode='bilinear', align_corners=False
        )
        
        # ì˜¤í”„ì…‹ ì ìš© (ì„œë¸Œì´ë¯¸ì§€ ìœ„ì¹˜ì— ë§ê²Œ)
        if offset_x > 0 or offset_y > 0:
            # ì˜¤í”„ì…‹ì´ ìˆëŠ” ê²½ìš° í•´ë‹¹ ì˜ì—­ë§Œ ìœ ì§€
            result = torch.zeros(1, 1, h, w, device=mask.device)
            sub_h, sub_w = mask.shape[-2:]
            result[:, :, offset_y:offset_y+sub_h, offset_x:offset_x+sub_w] = restored
            return result
        
        return restored
    
    def _fallback_original_image_processing(self, image_pil: Image.Image, target_class: str) -> Tuple[torch.Tensor, torch.Tensor]:
        """ì„œë¸Œì´ë¯¸ì§€ ì²˜ë¦¬ ì‹¤íŒ¨ ì‹œ ì›ë³¸ ì´ë¯¸ì§€ë¡œ fallback"""
        print("  ğŸ”„ Fallback: ì›ë³¸ ì´ë¯¸ì§€ë¡œ ì²˜ë¦¬")
        
        transform = transforms.Compose([
            transforms.Resize((1024, 1024)),
            transforms.ToTensor(),
        ])
        
        image_tensor = transform(image_pil).unsqueeze(0)
        
        # ê¸°ë³¸ VLM-SAM íŒŒì´í”„ë¼ì¸ ì‹¤í–‰
        occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels = self.extract_occlusion_info(
            image_pil, bbox=None, target_class=target_class, use_vlsam_method=True, auto_detect_bbox=True
        )
        
        if len(sam_points) > 0:
            amodal_mask, visible_mask = self._predict_sam_masks(image_tensor, sam_points, sam_labels)
        else:
            # ë¹ˆ ë§ˆìŠ¤í¬ ë°˜í™˜
            h, w = image_tensor.shape[-2:]
            amodal_mask = torch.zeros(1, 1, h, w)
            visible_mask = torch.zeros(1, 1, h, w)
        
        return amodal_mask, visible_mask
    
    def _predict_sam_masks(self, image: torch.Tensor, points: np.ndarray, labels: np.ndarray) -> Tuple[torch.Tensor, torch.Tensor]:
        """SAMì„ ì‚¬ìš©í•˜ì—¬ ë§ˆìŠ¤í¬ ì˜ˆì¸¡"""
        batch_size, _, img_h, img_w = image.shape
        
        # í¬ì¸íŠ¸ë¥¼ í…ì„œë¡œ ë³€í™˜
        points_tensor = torch.from_numpy(points).float().to(image.device)
        points_tensor = points_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, N, 2)
        
        labels_tensor = torch.from_numpy(labels).long().to(image.device)
        labels_tensor = labels_tensor.unsqueeze(0).unsqueeze(0)  # (1, 1, N)
        
        with torch.no_grad():
            # ì´ë¯¸ì§€ ì„ë² ë”©
            image_embeddings = self.image_encoder(image)
            image_embeddings = image_embeddings.detach()
            
            # í¬ì¸íŠ¸ ë¦¬ìŠ¤ì¼€ì¼ë§
            rescaled_points = self.efficient_sam.get_rescaled_pts(points_tensor, img_h, img_w)
            
            # Sparse embeddings ìƒì„±
            sparse_embeddings = self.prompt_encoder(
                rescaled_points.reshape(batch_size, len(points), 2),
                labels_tensor.reshape(batch_size, len(points)),
            )
            
            if len(sparse_embeddings.shape) == 3:
                sparse_embeddings = sparse_embeddings.unsqueeze(1)
            
            # Amodal ë§ˆìŠ¤í¬ ì˜ˆì¸¡
            amodal_logits, _ = self.amodal_mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=self.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                multimask_output=True,
            )
            
            # Visible ë§ˆìŠ¤í¬ ì˜ˆì¸¡
            visible_logits, _ = self.visible_mask_decoder(
                image_embeddings=image_embeddings,
                image_pe=self.prompt_encoder.get_dense_pe(),
                sparse_prompt_embeddings=sparse_embeddings,
                multimask_output=True,
            )
            
            # ë§ˆìŠ¤í¬ í›„ì²˜ë¦¬
            if len(amodal_logits.shape) == 5:
                amodal_mask = amodal_logits[:, 0, 0:1, :, :]
                visible_mask = visible_logits[:, 0, 0:1, :, :]
            elif len(amodal_logits.shape) == 4:
                amodal_mask = amodal_logits[:, 0:1, :, :]
                visible_mask = visible_logits[:, 0:1, :, :]
            else:
                raise ValueError(f"Unexpected logits shape: amodal={amodal_logits.shape}")
            
            # ì›ë³¸ ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
            if amodal_mask.shape[-2:] != (img_h, img_w):
                amodal_mask = F.interpolate(
                    amodal_mask, size=(img_h, img_w), mode='bilinear', align_corners=False
                )
            if visible_mask.shape[-2:] != (img_h, img_w):
                visible_mask = F.interpolate(
                    visible_mask, size=(img_h, img_w), mode='bilinear', align_corners=False
                )
            
            return amodal_mask, visible_mask
    
    def _per_sam_refinement(self, points: np.ndarray, labels: np.ndarray, 
                          amodal_mask: torch.Tensor, visible_mask: torch.Tensor) -> Tuple[np.ndarray, np.ndarray]:
        """PerSAM ë°©ì‹ì˜ cascaded post-refinement"""
        print("    ğŸ”„ PerSAM ë°©ì‹ ì ìš©: ë§ˆìŠ¤í¬ë¥¼ ì¶”ê°€ í”„ë¡¬í”„íŠ¸ë¡œ ì‚¬ìš©")
        
        # ë§ˆìŠ¤í¬ì—ì„œ ì¶”ê°€ í¬ì¸íŠ¸ ì¶”ì¶œ
        amodal_np = torch.sigmoid(amodal_mask[0]).squeeze().cpu().numpy()
        visible_np = torch.sigmoid(visible_mask[0]).squeeze().cpu().numpy()
        
        # ë§ˆìŠ¤í¬ ê²½ê³„ì—ì„œ í¬ì¸íŠ¸ ì¶”ì¶œ
        additional_points = self._extract_points_from_mask_boundary(amodal_np, visible_np)
        
        if len(additional_points) > 0:
            # ê¸°ì¡´ í¬ì¸íŠ¸ì™€ ê²°í•©
            combined_points = np.concatenate([points, additional_points], axis=0)
            combined_labels = np.concatenate([
                labels, 
                np.ones(len(additional_points), dtype=int)  # ì¶”ê°€ í¬ì¸íŠ¸ëŠ” positiveë¡œ ê°€ì •
            ], axis=0)
            
            print(f"      âœ“ ì¶”ê°€ í¬ì¸íŠ¸: {len(additional_points)}ê°œ (ì´ {len(combined_points)}ê°œ)")
            return combined_points, combined_labels
        else:
            print("      âš ï¸ ì¶”ê°€ í¬ì¸íŠ¸ ì—†ìŒ")
            return points, labels
    
    def _mask_attention_map(self, attention_map: np.ndarray, mask: torch.Tensor) -> np.ndarray:
        """ë§ˆìŠ¤í¬ë¥¼ ì‚¬ìš©í•˜ì—¬ attention mapì„ ë§ˆìŠ¤í‚¹"""
        mask_np = torch.sigmoid(mask[0]).squeeze().cpu().numpy()
        
        # ë§ˆìŠ¤í¬ ì˜ì—­ ë‚´ì˜ attentionë§Œ ìœ ì§€
        masked_attention = attention_map.copy()
        masked_attention[mask_np < 0.5] *= 0.1  # ë§ˆìŠ¤í¬ ì™¸ë¶€ëŠ” 10%ë¡œ ê°ì†Œ
        
        return masked_attention
    
    def _generate_points_from_masked_attention(self, masked_attention: np.ndarray, 
                                             current_points: np.ndarray, 
                                             current_labels: np.ndarray) -> Tuple[np.ndarray, np.ndarray]:
        """ë§ˆìŠ¤í¬ëœ attention mapì—ì„œ ìƒˆë¡œìš´ í¬ì¸íŠ¸ ìƒì„±"""
        print("    ğŸ¯ ë§ˆìŠ¤í¬ëœ attentionì—ì„œ í¬ì¸íŠ¸ ì¬ìƒì„±")
        
        # VL-SAM ë°©ì‹ìœ¼ë¡œ í¬ì¸íŠ¸ ìƒ˜í”Œë§
        positive_points, negative_points, labels = self.point_sampler.sample_points_from_attention(
            masked_attention,
            bbox=None,  # ë§ˆìŠ¤í¬ëœ attentionì´ë¯€ë¡œ bbox ë¶ˆí•„ìš”
            use_vlsam_method=True
        )
        
        if len(positive_points) > 0 and len(negative_points) > 0:
            all_points = np.concatenate([positive_points, negative_points], axis=0)
            all_labels = np.concatenate([
                np.ones(len(positive_points), dtype=int),
                np.zeros(len(negative_points), dtype=int)
            ], axis=0)
        elif len(positive_points) > 0:
            all_points = positive_points
            all_labels = np.ones(len(positive_points), dtype=int)
        elif len(negative_points) > 0:
            all_points = negative_points
            all_labels = np.zeros(len(negative_points), dtype=int)
        else:
            # Fallback: ê¸°ì¡´ í¬ì¸íŠ¸ ìœ ì§€
            all_points = current_points
            all_labels = current_labels
        
        return all_points, all_labels
    
    def _extract_points_from_mask_boundary(self, amodal_mask: np.ndarray, visible_mask: np.ndarray) -> np.ndarray:
        """ë§ˆìŠ¤í¬ ê²½ê³„ì—ì„œ í¬ì¸íŠ¸ ì¶”ì¶œ"""
        from scipy import ndimage
        
        # ë§ˆìŠ¤í¬ ê²½ê³„ ì°¾ê¸°
        amodal_boundary = self._find_mask_boundary(amodal_mask)
        visible_boundary = self._find_mask_boundary(visible_mask)
        
        # ê²½ê³„ í¬ì¸íŠ¸ë“¤ ìˆ˜ì§‘
        boundary_points = []
        
        if np.sum(amodal_boundary) > 0:
            amodal_points = np.column_stack(np.where(amodal_boundary))
            # ìƒ˜í”Œë§ (ìµœëŒ€ 5ê°œ)
            if len(amodal_points) > 5:
                indices = np.random.choice(len(amodal_points), 5, replace=False)
                amodal_points = amodal_points[indices]
            boundary_points.extend(amodal_points)
        
        if np.sum(visible_boundary) > 0:
            visible_points = np.column_stack(np.where(visible_boundary))
            # ìƒ˜í”Œë§ (ìµœëŒ€ 3ê°œ)
            if len(visible_points) > 3:
                indices = np.random.choice(len(visible_points), 3, replace=False)
                visible_points = visible_points[indices]
            boundary_points.extend(visible_points)
        
        if len(boundary_points) > 0:
            return np.array(boundary_points)
        else:
            return np.array([])
    
    def _find_mask_boundary(self, mask: np.ndarray, threshold: float = 0.5) -> np.ndarray:
        """ë§ˆìŠ¤í¬ì˜ ê²½ê³„ë¥¼ ì°¾ê¸°"""
        binary_mask = (mask > threshold).astype(np.uint8)
        
        # ê²½ê³„ ì°¾ê¸° (morphological gradient)
        from scipy import ndimage
        kernel = np.ones((3, 3), dtype=np.uint8)
        dilated = ndimage.binary_dilation(binary_mask, structure=kernel)
        eroded = ndimage.binary_erosion(binary_mask, structure=kernel)
        boundary = dilated.astype(np.uint8) - eroded.astype(np.uint8)
        
        return boundary > 0
    
    def _calculate_mask_iou(self, pred_mask: torch.Tensor, gt_mask: np.ndarray = None) -> float:
        """ë§ˆìŠ¤í¬ IoU ê³„ì‚° (GTê°€ ì—†ìœ¼ë©´ ê¸°ë³¸ê°’ ë°˜í™˜)"""
        if gt_mask is None:
            # GTê°€ ì—†ìœ¼ë©´ ë§ˆìŠ¤í¬ì˜ í‰ê· ê°’ì„ IoUë¡œ ì‚¬ìš©
            return float(torch.sigmoid(pred_mask).mean().item())
        else:
            # ì‹¤ì œ IoU ê³„ì‚°
            pred_binary = (torch.sigmoid(pred_mask) > 0.5).float()
            gt_tensor = torch.from_numpy(gt_mask).float().to(pred_mask.device)
            
            intersection = (pred_binary * gt_tensor).sum()
            union = pred_binary.sum() + gt_tensor.sum() - intersection
            
            if union > 0:
                return float(intersection / union)
            else:
                return 0.0
    
    def process_with_refinement(self, image_pil: Image.Image, target_class: str = None,
                              use_iterative_refinement: bool = True, use_multi_scale: bool = True,
                              num_iterations: int = 2, save_visualization: bool = True,
                              output_dir: str = "./outputs/d2sa_vlm_sam_250909") -> Dict:
        """
        VL-SAM ë…¼ë¬¸ì˜ Iterative Refinementì™€ Multi-scale Ensembleì„ í¬í•¨í•œ ì™„ì „í•œ íŒŒì´í”„ë¼ì¸
        
        Args:
            image_pil: ì…ë ¥ PIL ì´ë¯¸ì§€
            target_class: íƒ€ê²Ÿ ê°ì²´ í´ë˜ìŠ¤
            use_iterative_refinement: Iterative Refinement ì‚¬ìš© ì—¬ë¶€
            use_multi_scale: Multi-scale Ensemble ì‚¬ìš© ì—¬ë¶€
            num_iterations: ë°˜ë³µ íšŸìˆ˜
            save_visualization: ì‹œê°í™” ì €ì¥ ì—¬ë¶€
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict: ì²˜ë¦¬ ê²°ê³¼
        """
        print(f"ğŸš€ VL-SAM ì™„ì „ íŒŒì´í”„ë¼ì¸ ì‹œì‘")
        print(f"  - Iterative Refinement: {'í™œì„±í™”' if use_iterative_refinement else 'ë¹„í™œì„±í™”'}")
        print(f"  - Multi-scale Ensemble: {'í™œì„±í™”' if use_multi_scale else 'ë¹„í™œì„±í™”'}")
        print(f"  - ë°˜ë³µ íšŸìˆ˜: {num_iterations}")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        if save_visualization:
            os.makedirs(output_dir, exist_ok=True)
        
        try:
            if use_multi_scale:
                # Multi-scale Ensemble ì‚¬ìš©
                print("ğŸ” Multi-scale Ensemble ëª¨ë“œ")
                final_amodal, final_visible, ensemble_results = self.multi_scale_ensemble(
                    image_pil, target_class, num_iterations
                )
                
                # ê²°ê³¼ ì •ë¦¬
                result = {
                    "processing_mode": "multi_scale_ensemble",
                    "amodal_mask": final_amodal,
                    "visible_mask": final_visible,
                    "ensemble_results": ensemble_results,
                    "iterative_refinement": use_iterative_refinement,
                    "multi_scale": use_multi_scale,
                    "num_iterations": num_iterations,
                    "status": "success"
                }
                
            else:
                # ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ (Iterative Refinement í¬í•¨)
                print("ğŸ–¼ï¸ ë‹¨ì¼ ì´ë¯¸ì§€ ì²˜ë¦¬ ëª¨ë“œ")
                
                # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
                transform = transforms.Compose([
                    transforms.Resize((1024, 1024)),
                    transforms.ToTensor(),
                ])
                
                image_tensor = transform(image_pil).unsqueeze(0)
                
                # VLM ë¶„ì„ ë° attention map ì¶”ì¶œ
                occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels = self.extract_occlusion_info(
                    image_pil, bbox=None, target_class=target_class, use_vlsam_method=True, auto_detect_bbox=True
                )
                
                if len(sam_points) == 0:
                    raise RuntimeError("No SAM points generated")
                
                if use_iterative_refinement:
                    # Iterative Refinement ì ìš©
                    final_amodal, final_visible, refinement_results = self.iterative_refinement(
                        image_tensor, sam_points, sam_labels, aggregated_attention, num_iterations
                    )
                else:
                    # ê¸°ë³¸ SAM ë§ˆìŠ¤í¬ ì˜ˆì¸¡
                    final_amodal, final_visible = self._predict_sam_masks(image_tensor, sam_points, sam_labels)
                    refinement_results = []
                
                # ê²°ê³¼ ì •ë¦¬
                result = {
                    "processing_mode": "single_image",
                    "amodal_mask": final_amodal,
                    "visible_mask": final_visible,
                    "occlusion_info": occlusion_info,
                    "attention_maps": attention_maps,
                    "aggregated_attention": aggregated_attention,
                    "sam_points": sam_points,
                    "sam_labels": sam_labels,
                    "refinement_results": refinement_results,
                    "iterative_refinement": use_iterative_refinement,
                    "multi_scale": use_multi_scale,
                    "num_iterations": num_iterations,
                    "status": "success"
                }
            
            # ì‹œê°í™” ì €ì¥
            if save_visualization:
                import hashlib
                image_hash = hashlib.md5(str(image_pil.size).encode()).hexdigest()[:8]
                viz_filename = f"vl_sam_refined_{image_hash}_{target_class or 'unknown'}.png"
                viz_path = os.path.join(output_dir, viz_filename)
                
                # ì‹œê°í™” ìƒì„±
                self._create_refinement_visualization(
                    image_pil, result, viz_path, target_class
                )
                
                result["visualization_path"] = viz_path
                print(f"ğŸ¨ ì‹œê°í™” ì €ì¥: {viz_path}")
            
            print(f"âœ… VL-SAM ì™„ì „ íŒŒì´í”„ë¼ì¸ ì™„ë£Œ")
            return result
            
        except Exception as e:
            print(f"âŒ VL-SAM íŒŒì´í”„ë¼ì¸ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "processing_mode": "error",
                "error": str(e),
                "error_details": traceback.format_exc(),
                "status": "error"
            }
    
    def _create_refinement_visualization(self, image_pil: Image.Image, result: Dict, 
                                       save_path: str, target_class: str = None):
        """Refinement ê²°ê³¼ ì‹œê°í™”"""
        try:
            import matplotlib.pyplot as plt
            import matplotlib.patches as patches
            
            fig, axes = plt.subplots(2, 3, figsize=(18, 12))
            fig.suptitle(f'VL-SAM Refined Pipeline - {target_class or "Unknown Object"}', fontsize=16)
            
            # ì›ë³¸ ì´ë¯¸ì§€
            axes[0, 0].imshow(image_pil)
            axes[0, 0].set_title('Original Image')
            axes[0, 0].axis('off')
            
            # Amodal ë§ˆìŠ¤í¬
            if "amodal_mask" in result:
                amodal_np = torch.sigmoid(result["amodal_mask"][0]).squeeze().cpu().numpy()
                axes[0, 1].imshow(amodal_np, cmap='hot')
                axes[0, 1].set_title('Amodal Mask')
                axes[0, 1].axis('off')
            
            # Visible ë§ˆìŠ¤í¬
            if "visible_mask" in result:
                visible_np = torch.sigmoid(result["visible_mask"][0]).squeeze().cpu().numpy()
                axes[0, 2].imshow(visible_np, cmap='hot')
                axes[0, 2].set_title('Visible Mask')
                axes[0, 2].axis('off')
            
            # Attention Map
            if "aggregated_attention" in result:
                attention_np = result["aggregated_attention"]
                axes[1, 0].imshow(attention_np, cmap='viridis')
                axes[1, 0].set_title('Aggregated Attention')
                axes[1, 0].axis('off')
            
            # SAM Points
            if "sam_points" in result and "sam_labels" in result:
                axes[1, 1].imshow(image_pil)
                points = result["sam_points"]
                labels = result["sam_labels"]
                
                if len(points) > 0:
                    pos_points = points[labels == 1]
                    neg_points = points[labels == 0]
                    
                    if len(pos_points) > 0:
                        axes[1, 1].scatter(pos_points[:, 0], pos_points[:, 1], 
                                          c='red', s=100, marker='+', label='Positive')
                    if len(neg_points) > 0:
                        axes[1, 1].scatter(neg_points[:, 0], neg_points[:, 1], 
                                          c='blue', s=100, marker='x', label='Negative')
                
                axes[1, 1].set_title('SAM Points')
                axes[1, 1].axis('off')
            
            # Refinement ê²°ê³¼
            if "refinement_results" in result and len(result["refinement_results"]) > 0:
                iterations = [r["iteration"] for r in result["refinement_results"]]
                amodal_ious = [r["amodal_iou"] for r in result["refinement_results"]]
                visible_ious = [r["visible_iou"] for r in result["refinement_results"]]
                
                axes[1, 2].plot(iterations, amodal_ious, 'r-o', label='Amodal IoU')
                axes[1, 2].plot(iterations, visible_ious, 'b-s', label='Visible IoU')
                axes[1, 2].set_xlabel('Iteration')
                axes[1, 2].set_ylabel('IoU')
                axes[1, 2].set_title('Refinement Progress')
                axes[1, 2].legend()
                axes[1, 2].grid(True)
            else:
                axes[1, 2].text(0.5, 0.5, 'No Refinement Data', 
                               ha='center', va='center', transform=axes[1, 2].transAxes)
                axes[1, 2].set_title('Refinement Progress')
            
            plt.tight_layout()
            plt.savefig(save_path, dpi=150, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âŒ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
    
    def process_all_objects_sequentially(self, image_pil: Image.Image, 
                                       save_visualization: bool = True,
                                       output_dir: str = "./outputs/sequential_objects") -> Dict:
        """
        VLMì´ ì¶œë ¥í•œ ëª¨ë“  ê°ì²´(ê°€ë ¤ì§„ ê°ì²´ í¬í•¨)ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ segmentation ìˆ˜í–‰
        
        Args:
            image_pil: ì…ë ¥ PIL ì´ë¯¸ì§€
            save_visualization: ì‹œê°í™” ì €ì¥ ì—¬ë¶€
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict: ëª¨ë“  ê°ì²´ì— ëŒ€í•œ segmentation ê²°ê³¼
        """
        print(f"ğŸ”„ ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ ì‹œì‘: VLMì´ ê°ì§€í•œ ëª¨ë“  ê°ì²´ì— ëŒ€í•´ segmentation ìˆ˜í–‰")
        
        # ì¶œë ¥ ë””ë ‰í† ë¦¬ ìƒì„±
        if save_visualization:
            os.makedirs(output_dir, exist_ok=True)
        
        try:
            # 1. VLMì„ ì‚¬ìš©í•œ ì „ì²´ ê°ì²´ ë¶„ì„
            print("ğŸ” VLMì„ ì‚¬ìš©í•œ ì „ì²´ ê°ì²´ ë¶„ì„...")
            occlusion_info = self.occlusion_analyzer.analyze_occlusion(image_pil)
            
            # ëª¨ë“  ê°ì§€ëœ ê°ì²´ ìˆ˜ì§‘ (ê°€ë ¤ì§„ ê°ì²´ + ë³´ì´ëŠ” ê°ì²´)
            all_objects = []
            all_objects.extend(occlusion_info.get('visible_objects', []))
            all_objects.extend(occlusion_info.get('occluded_objects', []))
            
            # ì¤‘ë³µ ì œê±°
            unique_objects = list(set(all_objects))
            
            print(f"âœ“ VLMì´ ê°ì§€í•œ ê°ì²´: {len(unique_objects)}ê°œ")
            print(f"  - ë³´ì´ëŠ” ê°ì²´: {occlusion_info.get('visible_objects', [])}")
            print(f"  - ê°€ë ¤ì§„ ê°ì²´: {occlusion_info.get('occluded_objects', [])}")
            
            if len(unique_objects) == 0:
                print("âš ï¸ ê°ì§€ëœ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.")
                return {
                    "status": "no_objects",
                    "message": "VLMì´ ê°ì§€í•œ ê°ì²´ê°€ ì—†ìŠµë‹ˆë‹¤.",
                    "occlusion_info": occlusion_info
                }
            
            # 2. ê° ê°ì²´ì— ëŒ€í•´ ìˆœì°¨ì ìœ¼ë¡œ segmentation ìˆ˜í–‰
            object_results = []
            all_masks = []
            
            for i, object_name in enumerate(unique_objects):
                print(f"\\nğŸ¯ ê°ì²´ {i+1}/{len(unique_objects)} ì²˜ë¦¬: '{object_name}'")
                
                try:
                    # ê°œë³„ ê°ì²´ì— ëŒ€í•œ segmentation ìˆ˜í–‰
                    object_result = self._process_single_object(
                        image_pil, object_name, i, save_visualization, output_dir
                    )
                    
                    object_results.append(object_result)
                    
                    if object_result["status"] == "success":
                        all_masks.append({
                            "object_name": object_name,
                            "amodal_mask": object_result["amodal_mask"],
                            "visible_mask": object_result["visible_mask"],
                            "attention_map": object_result["attention_map"],
                            "sam_points": object_result["sam_points"]
                        })
                        print(f"  âœ… '{object_name}' segmentation ì„±ê³µ")
                    else:
                        print(f"  âŒ '{object_name}' segmentation ì‹¤íŒ¨: {object_result.get('error', 'Unknown')}")
                
                except Exception as e:
                    print(f"  âŒ '{object_name}' ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜: {e}")
                    object_results.append({
                        "object_name": object_name,
                        "status": "error",
                        "error": str(e)
                    })
            
            # 3. ì „ì²´ ê²°ê³¼ ì •ë¦¬
            successful_objects = [r for r in object_results if r["status"] == "success"]
            failed_objects = [r for r in object_results if r["status"] == "error"]
            
            print(f"\\nğŸ“Š ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ ì™„ë£Œ!")
            print(f"  - ì„±ê³µ: {len(successful_objects)}ê°œ")
            print(f"  - ì‹¤íŒ¨: {len(failed_objects)}ê°œ")
            print(f"  - ì„±ê³µë¥ : {len(successful_objects)/len(unique_objects)*100:.1f}%")
            
            # 4. í†µí•© ì‹œê°í™” ìƒì„±
            if save_visualization and len(all_masks) > 0:
                self._create_sequential_visualization(
                    image_pil, all_masks, object_results, output_dir
                )
            
            # 5. ìµœì¢… ê²°ê³¼ ë°˜í™˜
            result = {
                "status": "success",
                "total_objects": len(unique_objects),
                "successful_objects": len(successful_objects),
                "failed_objects": len(failed_objects),
                "success_rate": len(successful_objects)/len(unique_objects)*100,
                "occlusion_info": occlusion_info,
                "object_results": object_results,
                "all_masks": all_masks,
                "processing_mode": "sequential_all_objects"
            }
            
            return result
            
        except Exception as e:
            print(f"âŒ ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
            import traceback
            traceback.print_exc()
            
            return {
                "status": "error",
                "error": str(e),
                "error_details": traceback.format_exc()
            }
    
    def _process_single_object(self, image_pil: Image.Image, object_name: str, 
                             object_index: int, save_visualization: bool, 
                             output_dir: str) -> Dict:
        """
        ê°œë³„ ê°ì²´ì— ëŒ€í•œ segmentation ìˆ˜í–‰
        
        Args:
            image_pil: ì…ë ¥ ì´ë¯¸ì§€
            object_name: ì²˜ë¦¬í•  ê°ì²´ ì´ë¦„
            object_index: ê°ì²´ ì¸ë±ìŠ¤
            save_visualization: ì‹œê°í™” ì €ì¥ ì—¬ë¶€
            output_dir: ì¶œë ¥ ë””ë ‰í† ë¦¬
            
        Returns:
            Dict: ê°œë³„ ê°ì²´ segmentation ê²°ê³¼
        """
        try:
            # 1. ê°ì²´ë³„ attention map ì¶”ì¶œ
            print(f"    ğŸ§  '{object_name}'ì— ëŒ€í•œ attention map ì¶”ì¶œ...")
            
            # ê°ì²´ë³„ í”„ë¡¬í”„íŠ¸ ìƒì„±
            object_prompt = f"USER: <image>Focus on the {object_name} in this image. Describe its location, shape, and any visible parts.\nASSISTANT:"
            
            # VLMì„ ì‚¬ìš©í•œ ê°ì²´ë³„ attention map ì¶”ì¶œ
            attention_maps, aggregated_attention = self.occlusion_analyzer.extract_attention_maps(
                image_pil, prompt=object_prompt, use_vlsam_method=True
            )
            
            # 2. SAM prompt ìƒì„± (attention map ê¸°ë°˜)
            print(f"    ğŸ¯ '{object_name}'ì— ëŒ€í•œ SAM prompt ìƒì„±...")
            
            sam_points, sam_labels = self.occlusion_analyzer.generate_sam_prompts(
                image_pil, attention_maps, aggregated_attention, 
                bbox=None, target_class=object_name, use_vlsam_method=True
            )
            
            if len(sam_points) == 0:
                return {
                    "object_name": object_name,
                    "object_index": object_index,
                    "status": "error",
                    "error": "No SAM points generated"
                }
            
            # 3. SAMì„ ì‚¬ìš©í•œ ë§ˆìŠ¤í¬ ì˜ˆì¸¡
            print(f"    ğŸ­ '{object_name}'ì— ëŒ€í•œ ë§ˆìŠ¤í¬ ì˜ˆì¸¡...")
            
            # ì´ë¯¸ì§€ë¥¼ í…ì„œë¡œ ë³€í™˜
            transform = transforms.Compose([
                transforms.Resize((1024, 1024)),
                transforms.ToTensor(),
            ])
            
            image_tensor = transform(image_pil).unsqueeze(0)
            
            # SAM ë§ˆìŠ¤í¬ ì˜ˆì¸¡
            amodal_mask, visible_mask = self._predict_sam_masks(
                image_tensor, sam_points, sam_labels
            )
            
            # 4. ê²°ê³¼ ì •ë¦¬
            result = {
                "object_name": object_name,
                "object_index": object_index,
                "status": "success",
                "amodal_mask": amodal_mask,
                "visible_mask": visible_mask,
                "attention_map": aggregated_attention,
                "attention_maps": attention_maps,
                "sam_points": sam_points,
                "sam_labels": sam_labels,
                "amodal_iou": self._calculate_mask_iou(amodal_mask, None),
                "visible_iou": self._calculate_mask_iou(visible_mask, None)
            }
            
            # 5. ê°œë³„ ê°ì²´ ì‹œê°í™” (ì„ íƒì )
            if save_visualization:
                self._create_single_object_visualization(
                    image_pil, result, output_dir
                )
            
            return result
            
        except Exception as e:
            return {
                "object_name": object_name,
                "object_index": object_index,
                "status": "error",
                "error": str(e)
            }
    
    def _create_sequential_visualization(self, image_pil: Image.Image, all_masks: List[Dict], 
                                       object_results: List[Dict], output_dir: str):
        """ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ ê²°ê³¼ í†µí•© ì‹œê°í™”"""
        try:
            import matplotlib.pyplot as plt
            import matplotlib.patches as patches
            
            num_objects = len(all_masks)
            if num_objects == 0:
                return
            
            # ì„œë¸Œí”Œë¡¯ ë ˆì´ì•„ì›ƒ ê³„ì‚°
            cols = min(4, num_objects + 1)  # +1 for original image
            rows = (num_objects + 1 + cols - 1) // cols
            
            fig, axes = plt.subplots(rows, cols, figsize=(cols * 4, rows * 4))
            if rows == 1:
                axes = axes.reshape(1, -1)
            
            fig.suptitle(f'Sequential Object Processing - {num_objects} Objects', fontsize=16)
            
            # ì›ë³¸ ì´ë¯¸ì§€
            axes[0, 0].imshow(image_pil)
            axes[0, 0].set_title('Original Image')
            axes[0, 0].axis('off')
            
            # ê° ê°ì²´ë³„ ê²°ê³¼
            for i, mask_data in enumerate(all_masks):
                row = (i + 1) // cols
                col = (i + 1) % cols
                
                if row < rows and col < cols:
                    # Amodal ë§ˆìŠ¤í¬
                    amodal_np = torch.sigmoid(mask_data["amodal_mask"][0]).squeeze().cpu().numpy()
                    axes[row, col].imshow(amodal_np, cmap='hot')
                    axes[row, col].set_title(f'{mask_data["object_name"]}\\n(Amodal)')
                    axes[row, col].axis('off')
            
            # ë¹ˆ ì„œë¸Œí”Œë¡¯ ìˆ¨ê¸°ê¸°
            for i in range(num_objects + 1, rows * cols):
                row = i // cols
                col = i % cols
                if row < rows and col < cols:
                    axes[row, col].axis('off')
            
            plt.tight_layout()
            
            # ì €ì¥
            viz_path = os.path.join(output_dir, "sequential_objects_overview.png")
            plt.savefig(viz_path, dpi=150, bbox_inches='tight')
            plt.close()
            
            print(f"ğŸ¨ ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ ì‹œê°í™” ì €ì¥: {viz_path}")
            
        except Exception as e:
            print(f"âŒ ìˆœì°¨ì  ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")
    
    def _create_single_object_visualization(self, image_pil: Image.Image, result: Dict, output_dir: str):
        """ê°œë³„ ê°ì²´ ì‹œê°í™”"""
        try:
            import matplotlib.pyplot as plt
            
            fig, axes = plt.subplots(1, 3, figsize=(15, 5))
            fig.suptitle(f'Object: {result["object_name"]}', fontsize=14)
            
            # ì›ë³¸ ì´ë¯¸ì§€
            axes[0].imshow(image_pil)
            axes[0].set_title('Original Image')
            axes[0].axis('off')
            
            # Amodal ë§ˆìŠ¤í¬
            amodal_np = torch.sigmoid(result["amodal_mask"][0]).squeeze().cpu().numpy()
            axes[1].imshow(amodal_np, cmap='hot')
            axes[1].set_title('Amodal Mask')
            axes[1].axis('off')
            
            # Visible ë§ˆìŠ¤í¬
            visible_np = torch.sigmoid(result["visible_mask"][0]).squeeze().cpu().numpy()
            axes[2].imshow(visible_np, cmap='hot')
            axes[2].set_title('Visible Mask')
            axes[2].axis('off')
            
            plt.tight_layout()
            
            # ì €ì¥
            safe_name = result["object_name"].replace(" ", "_").replace("/", "_")
            viz_path = os.path.join(output_dir, f"object_{result['object_index']:02d}_{safe_name}.png")
            plt.savefig(viz_path, dpi=150, bbox_inches='tight')
            plt.close()
            
        except Exception as e:
            print(f"âŒ ê°œë³„ ê°ì²´ ì‹œê°í™” ìƒì„± ì‹¤íŒ¨: {e}")


# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == '__main__':
    print("=== VLM-SAM ëª¨ë¸ with D2SA ë°ì´í„°ì…‹ í…ŒìŠ¤íŠ¸ ì‹œì‘ ===")
    
    # VLM-SAM ëª¨ë¸ ì´ˆê¸°í™” (D2SA ë°ì´í„°ì…‹ í¬í•¨)
    print("ğŸ”§ ëª¨ë¸ ì´ˆê¸°í™” ì¤‘...")
    model = VLMSAMModel(use_d2sa=True)
    
    # íŒŒë¼ë¯¸í„° ìˆ˜ í™•ì¸
    total_params = sum(p.numel() for p in model.parameters())
    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)
    print(f"\nğŸ“ˆ ëª¨ë¸ ì •ë³´:")
    print(f"  - ì „ì²´ íŒŒë¼ë¯¸í„° ìˆ˜: {total_params:,}")
    print(f"  - í•™ìŠµ ê°€ëŠ¥í•œ íŒŒë¼ë¯¸í„° ìˆ˜: {trainable_params:,}")
    print(f"  - Zero-shot ëª¨ë“œ: {'âœ“' if trainable_params == 0 else 'âœ—'}")
    print(f"  - D2SA ë°ì´í„°ì…‹ ì‚¬ìš©: {'âœ“' if model.use_d2sa else 'âœ—'}")
    
    if model.use_d2sa and model.d2sa_dataset:
        print(f"  - D2SA ìƒ˜í”Œ ìˆ˜: {len(model.d2sa_dataset)}")
    
    # ë°©ë²• 1: ë‹¨ì¼ D2SA ìƒ˜í”Œ ì²˜ë¦¬
    print(f"\n=== ë°©ë²• 1: ë‹¨ì¼ D2SA ìƒ˜í”Œ ì²˜ë¦¬ ===")
    result = model.process_d2sa_sample(
        index=None,  # ëœë¤ ì„ íƒ
        save_visualization=True,
        save_vlm_analysis=True,
        output_dir="./outputs/d2sa_vlm_sam_250909"
    )
    
    if result["status"] == "success":
        print(f"âœ… ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬ ì„±ê³µ!")
        print(f"  - ì¹´í…Œê³ ë¦¬: {result['sample_info']['category_name']}")
        print(f"  - Amodal IoU: {result['processing_results']['pred_amodal_iou']:.3f}")
        print(f"  - Visible IoU: {result['processing_results']['pred_visible_iou']:.3f}")
        print(f"  - VLM Points: {result['processing_results']['num_sam_points']}ê°œ")
        print(f"  - ì‹œê°í™”: {result['visualization_path']}")
        print(f"  - VLM ë¶„ì„ JSON: {result['vlm_json_path']}")
    else:
        print(f"âŒ ë‹¨ì¼ ìƒ˜í”Œ ì²˜ë¦¬ ì‹¤íŒ¨: {result.get('error', 'Unknown error')}")
    
    # ë°©ë²• 2: ì—¬ëŸ¬ D2SA ìƒ˜í”Œ ë°°ì¹˜ ì²˜ë¦¬
    print(f"\n=== ë°©ë²• 2: ë°°ì¹˜ D2SA ìƒ˜í”Œ ì²˜ë¦¬ ===")
    batch_results = model.process_multiple_d2sa_samples(
        num_samples=3,
        save_vlm_analysis=True,
        output_dir="./outputs/d2sa_vlm_sam_250909"
    )
    
    print(f"âœ… ë°°ì¹˜ ì²˜ë¦¬ ì™„ë£Œ!")
    successful_results = [r for r in batch_results if r.get("status") == "success"]
    print(f"  - ì„±ê³µí•œ ìƒ˜í”Œ: {len(successful_results)}ê°œ")
    
    if successful_results:
        avg_amodal_iou = np.mean([r["processing_results"]["pred_amodal_iou"] for r in successful_results])
        avg_visible_iou = np.mean([r["processing_results"]["pred_visible_iou"] for r in successful_results])
        avg_points = np.mean([r["processing_results"]["num_sam_points"] for r in successful_results])
        
        print(f"  - í‰ê·  Amodal IoU: {avg_amodal_iou:.3f}")
        print(f"  - í‰ê·  Visible IoU: {avg_visible_iou:.3f}")
        print(f"  - í‰ê·  VLM Points: {avg_points:.1f}ê°œ")
    
    # ë°©ë²• 3: ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ (ìƒˆë¡œìš´ ê¸°ëŠ¥)
    print(f"\n=== ë°©ë²• 3: ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ (VLMì´ ê°ì§€í•œ ëª¨ë“  ê°ì²´) ===")
    
    # D2SA ìƒ˜í”Œ ì´ë¯¸ì§€ë¡œ ìˆœì°¨ì  ì²˜ë¦¬ í…ŒìŠ¤íŠ¸
    try:
        print("ğŸ“¸ D2SA ìƒ˜í”Œ ì´ë¯¸ì§€ë¡œ ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸...")
        d2sa_image_tensor, d2sa_bbox_tensor, d2sa_pil_image, d2sa_category_name, d2sa_ann_info = model.get_d2sa_sample(index=1)
        
        print(f"  - ì´ë¯¸ì§€ ID: {d2sa_ann_info['image_id']}")
        print(f"  - ì¹´í…Œê³ ë¦¬: {d2sa_category_name}")
        print(f"  - íŒŒì¼ëª…: {d2sa_ann_info['file_name']}")
        print(f"  - ì´ë¯¸ì§€ í¬ê¸°: {d2sa_pil_image.size}")
        
        # ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ ì‹¤í–‰
        sequential_result = model.process_all_objects_sequentially(
            image_pil=d2sa_pil_image,
            save_visualization=True,
            output_dir="./outputs/d2sa_vlm_sam_250909"
        )
        
        if sequential_result['status'] == 'success':
            print(f"âœ… ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ ì„±ê³µ!")
            print(f"  - ì´ ê°ì²´ ìˆ˜: {sequential_result['total_objects']}ê°œ")
            print(f"  - ì„±ê³µí•œ ê°ì²´: {sequential_result['successful_objects']}ê°œ")
            print(f"  - ì‹¤íŒ¨í•œ ê°ì²´: {sequential_result['failed_objects']}ê°œ")
            print(f"  - ì„±ê³µë¥ : {sequential_result['success_rate']:.1f}%")
            
            # VLMì´ ê°ì§€í•œ ê°ì²´ë“¤ ì¶œë ¥
            occlusion_info = sequential_result['occlusion_info']
            print(f"  - ë³´ì´ëŠ” ê°ì²´: {occlusion_info.get('visible_objects', [])}")
            print(f"  - ê°€ë ¤ì§„ ê°ì²´: {occlusion_info.get('occluded_objects', [])}")
            
            print(f"  - ì‹œê°í™” ì €ì¥: ./outputs/sequential_d2sa_test/")
        else:
            print(f"âŒ ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ ì‹¤íŒ¨: {sequential_result.get('error', 'Unknown')}")
            
    except Exception as e:
        print(f"âŒ ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬ í…ŒìŠ¤íŠ¸ ì‹¤íŒ¨: {e}")
    
    # ë°©ë²• 4: ê¸°ì¡´ ë°©ì‹ (ë”ë¯¸ ë°ì´í„°) - í˜¸í™˜ì„± ìœ ì§€
    print(f"\n=== ë°©ë²• 4: ê¸°ì¡´ ë”ë¯¸ ë°ì´í„° ë°©ì‹ (í˜¸í™˜ì„± í…ŒìŠ¤íŠ¸) ===")
    dummy_image = torch.randn(1, 3, 1024, 1024)
    dummy_box = torch.tensor([[100.0, 200.0, 300.0, 400.0]])
    dummy_pil = Image.new('RGB', (1024, 1024), color='lightgray')
    
    print("ğŸš€ ë”ë¯¸ ë°ì´í„°ë¡œ ì¶”ë¡  ì‹¤í–‰ ì¤‘...")
    amodal_mask, amodal_iou, visible_mask, visible_iou, occlusion_info, attention_maps, aggregated_attention, sam_points, sam_labels = model(
        dummy_image, dummy_box, dummy_pil, "a test object"
    )
    
    print(f"âœ… ë”ë¯¸ ë°ì´í„° ì¶”ë¡  ì™„ë£Œ!")
    print(f"  - Amodal ë§ˆìŠ¤í¬ shape: {amodal_mask.shape}")
    print(f"  - Visible ë§ˆìŠ¤í¬ shape: {visible_mask.shape}")
    print(f"  - Attention Maps: {len(attention_maps)}ê°œ layer")
    print(f"  - SAM Points: {sam_points.shape}")
    print(f"  - Positive Points: {np.sum(sam_labels)}ê°œ")
    
    print(f"\nğŸ¯ ì‚¬ìš©ë²• ìš”ì•½:")
    print(f"1. ë‹¨ì¼ D2SA ìƒ˜í”Œ: model.process_d2sa_sample()")
    print(f"2. ë°°ì¹˜ D2SA ìƒ˜í”Œ: model.process_multiple_d2sa_samples()")
    print(f"3. ìˆœì°¨ì  ê°ì²´ ì²˜ë¦¬: model.process_all_objects_sequentially()")
    print(f"4. íŠ¹ì • D2SA ìƒ˜í”Œ: model.get_d2sa_sample(index=N)")
    print(f"5. ê¸°ì¡´ ë°©ì‹ í˜¸í™˜: model(image, box, pil_image, text)")
    
    print("\n=== VLM-SAM ëª¨ë¸ with D2SA í…ŒìŠ¤íŠ¸ ì™„ë£Œ ===")
