"""
VLM (LLaVA) decoderì—ì„œ attention mapì„ ì¶”ì¶œí•˜ëŠ” ëª¨ë“ˆ
Multi-layer multi-head attentionì„ í†µí•´ attention mapì„ ìƒì„±í•©ë‹ˆë‹¤.
"""

import torch
import torch.nn as nn
import torch.nn.functional as F
import numpy as np
from typing import Dict, List, Tuple, Optional
import cv2
from PIL import Image

class AttentionExtractor:
    """
    VLM decoderì—ì„œ attention mapì„ ì¶”ì¶œí•˜ëŠ” í´ë˜ìŠ¤
    """
    
    def __init__(self, model, processor):
        """
        Args:
            model: LLaVA ëª¨ë¸ (í•„ìˆ˜)
            processor: LLaVA processor (í•„ìˆ˜)
        """
        if model is None or processor is None:
            raise ValueError("AttentionExtractorëŠ” ìœ íš¨í•œ LLaVA ëª¨ë¸ê³¼ processorê°€ í•„ìˆ˜ì…ë‹ˆë‹¤!")
        
        self.model = model
        self.processor = processor
        self.device = next(model.parameters()).device
        
        # Attention ì €ì¥ì„ ìœ„í•œ hooks
        self.attention_maps = {}
        self.attention_hooks = []
        
        # ëª¨ë¸ êµ¬ì¡° ë¶„ì„
        self._analyze_model_structure()
        
        print("AttentionExtractor ì´ˆê¸°í™” ì™„ë£Œ")
    
    def _analyze_model_structure(self):
        """ëª¨ë¸ êµ¬ì¡°ë¥¼ ë¶„ì„í•˜ì—¬ attention layerë¥¼ ì°¾ìŠµë‹ˆë‹¤."""
        print("=== LLaVA ëª¨ë¸ êµ¬ì¡° ë¶„ì„ ===")
        
        # LLaVA ëª¨ë¸ì˜ ì£¼ìš” ì»´í¬ë„ŒíŠ¸ í™•ì¸
        if hasattr(self.model, 'language_model'):
            self.language_model = self.model.language_model
            print(f"âœ“ Language Model: {type(self.language_model).__name__}")
        
        if hasattr(self.model, 'vision_tower'):
            self.vision_tower = self.model.vision_tower
            print(f"âœ“ Vision Tower: {type(self.vision_tower).__name__}")
        
        if hasattr(self.model, 'multi_modal_projector'):
            self.projector = self.model.multi_modal_projector
            print(f"âœ“ Multi-modal Projector: {type(self.projector).__name__}")
        
        # Transformer layers ì°¾ê¸°
        self.transformer_layers = []
        if hasattr(self.language_model, 'model') and hasattr(self.language_model.model, 'layers'):
            self.transformer_layers = self.language_model.model.layers
            print(f"âœ“ Transformer Layers: {len(self.transformer_layers)}ê°œ")
        else:
            # LLaVA 1.5ì˜ ë‹¤ë¥¸ êµ¬ì¡° ì‹œë„
            if hasattr(self.model, 'language_model') and hasattr(self.model.language_model, 'layers'):
                self.transformer_layers = self.model.language_model.layers
                print(f"âœ“ Transformer Layers (direct): {len(self.transformer_layers)}ê°œ")
            elif hasattr(self.model, 'model') and hasattr(self.model.model, 'layers'):
                self.transformer_layers = self.model.model.layers
                print(f"âœ“ Transformer Layers (model.model): {len(self.transformer_layers)}ê°œ")
        
        # Vision Transformer layers ì°¾ê¸° (ë§Œì•½ ìˆë‹¤ë©´)
        self.vision_layers = []
        if hasattr(self.vision_tower, 'vision_model') and hasattr(self.vision_tower.vision_model, 'encoder'):
            if hasattr(self.vision_tower.vision_model.encoder, 'layers'):
                self.vision_layers = self.vision_tower.vision_model.encoder.layers
                print(f"âœ“ Vision Transformer Layers: {len(self.vision_layers)}ê°œ")
    
    def register_attention_hooks(self, layer_indices: Optional[List[int]] = None):
        """
        Attention ì¶”ì¶œì„ ìœ„í•œ hooksë¥¼ ë“±ë¡í•©ë‹ˆë‹¤.
        
        Args:
            layer_indices: ì¶”ì¶œí•  layer ì¸ë±ìŠ¤ë“¤ (Noneì´ë©´ ëª¨ë“  layer)
        """
        # ê¸°ì¡´ hooks ì œê±°
        self.remove_attention_hooks()
        self.attention_maps = {}
        
        if layer_indices is None:
            # ë§ˆì§€ë§‰ ëª‡ ê°œ layerë§Œ ì‚¬ìš© (ê³„ì‚° íš¨ìœ¨ì„±ì„ ìœ„í•´)
            layer_indices = list(range(max(0, len(self.transformer_layers) - 6), len(self.transformer_layers)))
        
        print(f"Attention hooks ë“±ë¡ ì¤‘: layers {layer_indices}")
        
        for i, layer_idx in enumerate(layer_indices):
            if layer_idx < len(self.transformer_layers):
                layer = self.transformer_layers[layer_idx]
                
                # Self-attention ëª¨ë“ˆ ì°¾ê¸°
                if hasattr(layer, 'self_attn'):
                    hook = layer.self_attn.register_forward_hook(
                        self._create_attention_hook(f"layer_{layer_idx}")
                    )
                    self.attention_hooks.append(hook)
                    print(f"  âœ“ Layer {layer_idx}: Self-attention hook ë“±ë¡")
    
    def _create_attention_hook(self, layer_name: str):
        """Attention mapì„ ì¶”ì¶œí•˜ëŠ” hook í•¨ìˆ˜ë¥¼ ìƒì„±í•©ë‹ˆë‹¤."""
        def hook_fn(module, input, output):
            try:
                # Multi-head attentionì˜ ì¶œë ¥ì—ì„œ attention weights ì¶”ì¶œ
                if isinstance(output, tuple) and len(output) > 1:
                    # output[0]: attention output, output[1]: attention weights
                    attention_weights = output[1]  # (batch_size, num_heads, seq_len, seq_len)
                    
                    if attention_weights is not None:
                        # CPUë¡œ ì´ë™í•˜ì—¬ ì €ì¥
                        self.attention_maps[layer_name] = attention_weights.detach().cpu()
                        print(f"    Attention map ì €ì¥: {layer_name}, shape: {attention_weights.shape}")
                
            except Exception as e:
                print(f"    Attention ì¶”ì¶œ ì‹¤íŒ¨ ({layer_name}): {e}")
        
        return hook_fn
    
    def remove_attention_hooks(self):
        """ë“±ë¡ëœ ëª¨ë“  hooksë¥¼ ì œê±°í•©ë‹ˆë‹¤."""
        for hook in self.attention_hooks:
            hook.remove()
        self.attention_hooks = []
        print("ëª¨ë“  attention hooks ì œê±°ë¨")
    
    def extract_attention_maps(self, image: Image.Image, prompt: str = None, use_vlsam_method: bool = True) -> Dict[str, torch.Tensor]:
        """
        ì´ë¯¸ì§€ì™€ í…ìŠ¤íŠ¸ì— ëŒ€í•´ attention mapì„ ì¶”ì¶œí•©ë‹ˆë‹¤.
        
        Args:
            image: PIL ì´ë¯¸ì§€
            prompt: í…ìŠ¤íŠ¸ í”„ë¡¬í”„íŠ¸
            use_vlsam_method: VL-SAM ë…¼ë¬¸ ë°©ì‹ ì‚¬ìš© ì—¬ë¶€
            
        Returns:
            Dict: layerë³„ attention maps
        """
        if prompt is None:
            prompt = "Describe what you see in this image, focusing on objects and their relationships."
        
        print(f"ğŸ” VLM Attention map ì¶”ì¶œ ì‹œì‘ ({'VL-SAM ë°©ì‹' if use_vlsam_method else 'ê¸°ë³¸ ë°©ì‹'})...")
        print(f"  - í”„ë¡¬í”„íŠ¸: {prompt}")
        
        if use_vlsam_method:
            return self.extract_vlsam_attention_maps(image, prompt)
        else:
            return self.extract_basic_attention_maps(image, prompt)
    
    def extract_basic_attention_maps(self, image: Image.Image, prompt: str) -> Dict[str, torch.Tensor]:
        """ê¸°ì¡´ ë°©ì‹ì˜ attention map ì¶”ì¶œ"""
        # Attention hooks ë“±ë¡
        self.register_attention_hooks()
        
        try:
            # LLaVA 1.5 í˜•ì‹ì— ë§ëŠ” í”„ë¡¬í”„íŠ¸ë¡œ ë³€í™˜ (main_prompt_VLM_reasoning_250704.py ë°©ì‹)
            llava_prompt = f"USER: <image>{prompt}\nASSISTANT:"
            
            inputs = self.processor(
                text=llava_prompt, 
                images=image, 
                return_tensors="pt"
            ).to(self.device)
            
            # ëª¨ë¸ ì¶”ë¡  (attention map ì¶”ì¶œ) - main_prompt_VLM_reasoning_250704.py ë°©ì‹
            with torch.no_grad():
                generate_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=50,  # ì§§ê²Œ ì„¤ì •
                    temperature=0.0     # ê²€ì¦ëœ ì„¤ì •
                )
            
            # ì¶”ì¶œëœ attention maps ë°˜í™˜
            if self.attention_maps:
                print(f"  âœ“ ì‹¤ì œ attention map ì¶”ì¶œ ì™„ë£Œ: {len(self.attention_maps)}ê°œ layer")
                return self.attention_maps
            else:
                print("âŒ Attention mapì´ ì¶”ì¶œë˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
                print("âŒ VLMì—ì„œ ì‹¤ì œ attention mapì„ ì¶”ì¶œí•  ìˆ˜ ì—†ì–´ í”„ë¡œê·¸ë¨ì„ ì¤‘ë‹¨í•©ë‹ˆë‹¤.")
                raise RuntimeError("VLM attention map ì¶”ì¶œ ì‹¤íŒ¨: attention hooksê°€ ì •ìƒì ìœ¼ë¡œ ì‘ë™í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
        except Exception as e:
            print(f"âŒ ì‹¤ì œ VLM attention map ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            raise RuntimeError(f"VLM attention map ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        finally:
            # Hooks ì •ë¦¬
            self.remove_attention_hooks()
    
    def extract_vlsam_attention_maps(self, image: Image.Image, prompt: str) -> Dict[str, torch.Tensor]:
        """
        VL-SAM ë…¼ë¬¸ ë°©ì‹ì˜ attention map ì¶”ì¶œ (ìˆ˜ì •ëœ ë²„ì „)
        - ë” ê¸´ í…ìŠ¤íŠ¸ ìƒì„±ìœ¼ë¡œ ì¶©ë¶„í•œ attention ì •ë³´ í™•ë³´
        - ì´ë¯¸ì§€-í…ìŠ¤íŠ¸ Cross-Attention í™œìš©
        """
        print("  ğŸ§  VL-SAM ë°©ì‹ ì ìš©: Attention Flow + Rollout (ê°œì„ ëœ ë²„ì „)")
        
        # Query-Key ì¶”ì¶œì„ ìœ„í•œ hooks ë“±ë¡
        self.register_vlsam_hooks()
        
        try:
            # ë” ìƒì„¸í•œ í”„ë¡¬í”„íŠ¸ë¡œ ë” ë§ì€ í† í° ìƒì„± ìœ ë„
            vlsam_prompt = f"USER: <image>Describe all objects in this image in detail, including their locations, colors, shapes, and relationships with other objects. List at least 10 different elements you can see.\nASSISTANT:"
            
            inputs = self.processor(
                text=vlsam_prompt, 
                images=image, 
                return_tensors="pt"
            ).to(self.device)
            
            print(f"  ğŸ“Š Input ì •ë³´:")
            print(f"    - Input IDs shape: {inputs.input_ids.shape}")
            if hasattr(inputs, 'pixel_values'):
                print(f"    - Pixel values shape: {inputs.pixel_values.shape}")
            
            # ì¶©ë¶„í•œ í† í° ìƒì„±ìœ¼ë¡œ ë” ë§ì€ attention ì •ë³´ í™•ë³´
            with torch.no_grad():
                generate_ids = self.model.generate(
                    **inputs,
                    max_new_tokens=200,  # ë” ë§ì€ í† í° ìƒì„±
                    min_new_tokens=50,   # ìµœì†Œ í† í° ìˆ˜ ë³´ì¥
                    temperature=0.1,     # ì•½ê°„ì˜ ë‹¤ì–‘ì„±
                    do_sample=True,      # ìƒ˜í”Œë§ í™œì„±í™”
                    return_dict_in_generate=True,
                    output_attentions=True
                )
            
            print(f"  ğŸ“ˆ Generated tokens: {generate_ids.sequences.shape[1] - inputs.input_ids.shape[1]}ê°œ")
            
            # VL-SAM Attention Flow ì²˜ë¦¬
            if not hasattr(self, 'attention_weights_cache') or not self.attention_weights_cache:
                raise RuntimeError("VL-SAM Hook ì‹¤íŒ¨: attention_weights_cacheê°€ ë¹„ì–´ìˆìŒ. Hookì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
            
            # ìºì‹œëœ attention ì •ë³´ ë””ë²„ê¹…
            print(f"  ğŸ” Attention Cache ì •ë³´:")
            for layer_name, attention in self.attention_weights_cache.items():
                print(f"    - {layer_name}: {attention.shape}")
            
            attention_maps = self.compute_vlsam_attention_flow()
            print(f"  âœ“ VL-SAM Attention Flow ì™„ë£Œ: {len(attention_maps)}ê°œ layer")
            return attention_maps
            
        except Exception as e:
            print(f"âŒ VL-SAM attention map ì¶”ì¶œ ì‹¤íŒ¨: {e}")
            print("âŒ ë”ë¯¸ ë°ì´í„° ì‚¬ìš© ê¸ˆì§€: VL-SAM Hookì´ ë°˜ë“œì‹œ ì‘ë™í•´ì•¼ í•©ë‹ˆë‹¤.")
            import traceback
            traceback.print_exc()
            raise RuntimeError(f"VL-SAM Hook í•„ìˆ˜ ì‹¤íŒ¨: {e}")
        
        finally:
            # Hooks ì •ë¦¬
            self.remove_vlsam_hooks()
    
    def register_vlsam_hooks(self):
        """VL-SAMì„ ìœ„í•œ Query-Key ì¶”ì¶œ hooks ë“±ë¡ (ê°•í™”ëœ ë²„ì „)"""
        # ê¸°ì¡´ hooks ì œê±°
        self.remove_attention_hooks()
        
        # Query-Key ìºì‹œ ì´ˆê¸°í™”
        self.queries_cache = {}
        self.keys_cache = {}
        self.attention_weights_cache = {}
        
        # ë§ˆì§€ë§‰ ëª‡ ê°œ layerì—ë§Œ hook ë“±ë¡ (ê³„ì‚° íš¨ìœ¨ì„±)
        layer_indices = list(range(max(0, len(self.transformer_layers) - 6), len(self.transformer_layers)))
        
        print(f"VL-SAM Query-Key hooks ë“±ë¡ ì¤‘: layers {layer_indices}")
        print(f"ì „ì²´ transformer layers: {len(self.transformer_layers)}ê°œ")
        
        for layer_idx in layer_indices:
            if layer_idx < len(self.transformer_layers):
                layer = self.transformer_layers[layer_idx]
                
                # ë‹¤ì–‘í•œ attention module êµ¬ì¡°ì— ëŒ€ì‘
                attention_module = None
                module_type = None
                
                if hasattr(layer, 'self_attn'):
                    attention_module = layer.self_attn
                    module_type = "self_attn"
                elif hasattr(layer, 'attention'):
                    attention_module = layer.attention
                    module_type = "attention"
                elif hasattr(layer, 'attn'):
                    attention_module = layer.attn
                    module_type = "attn"
                
                if attention_module is not None:
                    # ëª¨ë“ˆ êµ¬ì¡° ë¶„ì„
                    print(f"  ğŸ“‹ Layer {layer_idx} ({module_type}): {type(attention_module).__name__}")
                    
                    # Hook ë“±ë¡
                    hook = attention_module.register_forward_hook(
                        self._create_vlsam_hook(layer_idx, module_type)
                    )
                    self.attention_hooks.append(hook)
                    print(f"  âœ“ Layer {layer_idx}: VL-SAM hook ë“±ë¡ ì„±ê³µ")
                else:
                    print(f"  âŒ Layer {layer_idx}: attention moduleì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
    
    def _create_vlsam_hook(self, layer_idx: int, module_type: str = "self_attn"):
        """VL-SAMì„ ìœ„í•œ Query-Key ì¶”ì¶œ hook (ì™„ì „ ê°•í™” ë²„ì „)"""
        def vlsam_hook_fn(module, input, output):
            try:
                # 1. Input êµ¬ì¡° ì™„ì „ ë¶„ì„
                hidden_states = None
                attention_mask = None
                
                # ë‹¤ì–‘í•œ input íŒ¨í„´ì— ëŒ€í•œ ì™„ì „í•œ ëŒ€ì‘
                if isinstance(input, tuple):
                    if len(input) >= 1:
                        hidden_states = input[0]
                    if len(input) >= 2 and input[1] is not None:
                        attention_mask = input[1]
                elif isinstance(input, torch.Tensor):
                    hidden_states = input
                elif isinstance(input, dict):
                    hidden_states = input.get('hidden_states', input.get('input', None))
                    attention_mask = input.get('attention_mask', None)
                
                # hidden_states ê²€ì¦ ë° ë³µêµ¬ ì‹œë„
                if hidden_states is None:
                    # outputì—ì„œ ì—­ì¶”ë¡  ì‹œë„
                    if isinstance(output, tuple) and len(output) > 0:
                        hidden_states = output[0]
                    elif isinstance(output, torch.Tensor):
                        hidden_states = output
                
                if hidden_states is None or not isinstance(hidden_states, torch.Tensor):
                    # ëª¨ë“ˆì˜ í˜„ì¬ ìƒíƒœì—ì„œ ì¶”ì¶œ ì‹œë„
                    if hasattr(module, 'last_hidden_state'):
                        hidden_states = module.last_hidden_state
                    else:
                        raise ValueError(f"Layer {layer_idx}: hidden_statesë¥¼ ì°¾ì„ ìˆ˜ ì—†ìŒ")
                
                # print(f"    âœ“ Layer {layer_idx}: hidden_states shape={hidden_states.shape}")
                
                # 2. Query, Key ì¶”ì¶œ - ë‹¤ì–‘í•œ ëª¨ë“ˆ êµ¬ì¡°ì— ëŒ€ì‘
                query, key = self._extract_query_key_from_module(module, hidden_states, layer_idx)
                
                if query is None or key is None:
                    raise ValueError(f"Layer {layer_idx}: Query/Key ì¶”ì¶œ ì‹¤íŒ¨")
                
                # 3. Multi-head attention parameters ì¶”ì¶œ
                num_heads, head_dim = self._get_attention_params(module, query, layer_idx)
                
                batch_size, seq_len, _ = query.shape
                
                # 4. Multi-head reshape (ì•ˆì „í•œ ë²„ì „)
                try:
                    query_reshaped = query.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)  # (B, H, N, D)
                    key_reshaped = key.view(batch_size, seq_len, num_heads, head_dim).transpose(1, 2)      # (B, H, N, D)
                except Exception as e:
                    # Fallback: ê°„ë‹¨í•œ reshape
                    print(f"    âš ï¸ Layer {layer_idx}: í‘œì¤€ reshape ì‹¤íŒ¨, ë‹¨ìˆœí™”ëœ ë°©ì‹ ì‚¬ìš©")
                    query_reshaped = query.unsqueeze(1)  # (B, 1, N, D)
                    key_reshaped = key.unsqueeze(1)      # (B, 1, N, D)
                    num_heads = 1
                    head_dim = query.shape[-1]
                
                # 5. VL-SAM ë…¼ë¬¸ì˜ ìœ ì‚¬ë„ í–‰ë ¬ ê³„ì‚°
                similarity = torch.matmul(query_reshaped, key_reshaped.transpose(-2, -1))  # (B, H, N, N)
                
                # Scale factor ì ìš©
                similarity = similarity / (head_dim ** 0.5)
                
                # 6. Causal mask ì ìš© (VL-SAM ë…¼ë¬¸ ë°©ì‹)
                if attention_mask is not None:
                    # ì‹¤ì œ attention mask ì‚¬ìš©
                    similarity = similarity.masked_fill(attention_mask == 0, float('-inf'))
                else:
                    # ê¸°ë³¸ causal mask
                    causal_mask = torch.tril(torch.ones(seq_len, seq_len, device=similarity.device))
                    similarity = similarity.masked_fill(causal_mask == 0, float('-inf'))
                
                # 7. SoftMax ì •ê·œí™” (VL-SAM ë…¼ë¬¸ ê³µì‹)
                attention_weights = torch.softmax(similarity, dim=-1)  # (B, H, N, N)
                
                # 8. ìºì‹œì— ì €ì¥
                self.queries_cache[f"layer_{layer_idx}"] = query_reshaped.detach().cpu()
                self.keys_cache[f"layer_{layer_idx}"] = key_reshaped.detach().cpu()
                self.attention_weights_cache[f"layer_{layer_idx}"] = attention_weights.detach().cpu()
                
                # print(f"    âœ… VL-SAM ë°ì´í„° ìºì‹œ ì„±ê³µ: layer_{layer_idx}")
                # print(f"        - Q/K shape: {query_reshaped.shape}")
                # print(f"        - Attention shape: {attention_weights.shape}")
                # print(f"        - Heads: {num_heads}, Head_dim: {head_dim}")
                
            except Exception as e:
                print(f"    âŒ VL-SAM hook ì¹˜ëª…ì  ì‹¤íŒ¨ (layer_{layer_idx}): {e}")
                import traceback
                traceback.print_exc()
                
                # ì¹˜ëª…ì  ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë°œìƒ (ë”ë¯¸ ë°ì´í„° ë°©ì§€)
                raise RuntimeError(f"VL-SAM Hook layer_{layer_idx} í•„ìˆ˜ ì‹¤íŒ¨: {e}")
        
        return vlsam_hook_fn
    
    def _extract_query_key_from_module(self, module, hidden_states: torch.Tensor, layer_idx: int):
        """ëª¨ë“ˆì—ì„œ Query, Key ì¶”ì¶œ (ë‹¤ì–‘í•œ êµ¬ì¡° ì§€ì›)"""
        try:
            # ë°©ë²• 1: í‘œì¤€ q_proj, k_proj
            if hasattr(module, 'q_proj') and hasattr(module, 'k_proj'):
                query = module.q_proj(hidden_states)
                key = module.k_proj(hidden_states)
                # print(f"    âœ“ Layer {layer_idx}: q_proj/k_proj ë°©ì‹ ì‚¬ìš©")
                return query, key
            
            # ë°©ë²• 2: query, key ì†ì„±
            elif hasattr(module, 'query') and hasattr(module, 'key'):
                query = module.query(hidden_states)
                key = module.key(hidden_states)
                # print(f"    âœ“ Layer {layer_idx}: query/key ë°©ì‹ ì‚¬ìš©")
                return query, key
            
            # ë°©ë²• 3: Linear layerë“¤ íƒìƒ‰
            elif hasattr(module, 'linear_q') and hasattr(module, 'linear_k'):
                query = module.linear_q(hidden_states)
                key = module.linear_k(hidden_states)
                # print(f"    âœ“ Layer {layer_idx}: linear_q/linear_k ë°©ì‹ ì‚¬ìš©")
                return query, key
            
            # ë°©ë²• 4: í•˜ìœ„ ëª¨ë“ˆ íƒìƒ‰
            else:
                for name, submodule in module.named_children():
                    if 'q' in name.lower() or 'query' in name.lower():
                        q_module = submodule
                    elif 'k' in name.lower() or 'key' in name.lower():
                        k_module = submodule
                
                if 'q_module' in locals() and 'k_module' in locals():
                    query = q_module(hidden_states)
                    key = k_module(hidden_states)
                    print(f"    âœ“ Layer {layer_idx}: í•˜ìœ„ ëª¨ë“ˆ íƒìƒ‰ ë°©ì‹ ì‚¬ìš©")
                    return query, key
            
            print(f"    âŒ Layer {layer_idx}: Query/Key ì¶”ì¶œ ë°©ë²•ì„ ì°¾ì„ ìˆ˜ ì—†ìŒ")
            return None, None
            
        except Exception as e:
            print(f"    âŒ Layer {layer_idx}: Query/Key ì¶”ì¶œ ì¤‘ ì˜¤ë¥˜: {e}")
            return None, None
    
    def _get_attention_params(self, module, query: torch.Tensor, layer_idx: int):
        """Attention íŒŒë¼ë¯¸í„° ì¶”ì¶œ (num_heads, head_dim)"""
        try:
            # ë°©ë²• 1: ëª¨ë“ˆ ì†ì„±ì—ì„œ ì§ì ‘ ì¶”ì¶œ
            if hasattr(module, 'num_heads') and hasattr(module, 'head_dim'):
                return module.num_heads, module.head_dim
            elif hasattr(module, 'num_attention_heads') and hasattr(module, 'attention_head_size'):
                return module.num_attention_heads, module.attention_head_size
            elif hasattr(module, 'n_heads') and hasattr(module, 'head_size'):
                return module.n_heads, module.head_size
            
            # ë°©ë²• 2: configì—ì„œ ì¶”ì¶œ
            elif hasattr(module, 'config'):
                config = module.config
                if hasattr(config, 'num_attention_heads') and hasattr(config, 'hidden_size'):
                    num_heads = config.num_attention_heads
                    head_dim = config.hidden_size // num_heads
                    return num_heads, head_dim
            
            # ë°©ë²• 3: ëª¨ë¸ ì „ì²´ configì—ì„œ ì¶”ì¶œ
            elif hasattr(self.model, 'config'):
                config = self.model.config
                if hasattr(config, 'num_attention_heads') and hasattr(config, 'hidden_size'):
                    num_heads = config.num_attention_heads
                    head_dim = config.hidden_size // num_heads
                    return num_heads, head_dim
            
            # ë°©ë²• 4: Query tensor shapeì—ì„œ ì¶”ì •
            else:
                hidden_size = query.shape[-1]
                # LLaMA ê³„ì—´ ëª¨ë¸ì˜ ì¼ë°˜ì ì¸ ì„¤ì • ì¶”ì •
                if hidden_size == 4096:
                    num_heads = 32
                elif hidden_size == 2048:
                    num_heads = 16
                elif hidden_size == 1024:
                    num_heads = 8
                else:
                    num_heads = max(1, hidden_size // 128)  # ì¶”ì •ê°’
                
                head_dim = hidden_size // num_heads
                print(f"    ğŸ“Š Layer {layer_idx}: íŒŒë¼ë¯¸í„° ì¶”ì • - heads={num_heads}, head_dim={head_dim}")
                return num_heads, head_dim
                
        except Exception as e:
            print(f"    âš ï¸ Layer {layer_idx}: íŒŒë¼ë¯¸í„° ì¶”ì¶œ ì‹¤íŒ¨, ê¸°ë³¸ê°’ ì‚¬ìš©: {e}")
            # ìµœí›„ì˜ ê¸°ë³¸ê°’
            return 1, query.shape[-1]
    
    def remove_vlsam_hooks(self):
        """VL-SAM hooks ì œê±°"""
        self.remove_attention_hooks()
        
        # ìºì‹œ ì •ë¦¬
        if hasattr(self, 'queries_cache'):
            del self.queries_cache
        if hasattr(self, 'keys_cache'):
            del self.keys_cache
        if hasattr(self, 'attention_weights_cache'):
            del self.attention_weights_cache
        
        print("VL-SAM hooks ë° ìºì‹œ ì •ë¦¬ ì™„ë£Œ")
    
    def compute_vlsam_attention_flow(self) -> Dict[str, torch.Tensor]:
        """
        VL-SAM ë…¼ë¬¸ì˜ Attention Flow ê³„ì‚°
        1. Mean-Max Attention Head Weights ê³„ì‚°
        2. Head ì§‘ê³„
        3. Attention Rollout with Regularization
        """
        print("  ğŸ“Š VL-SAM Attention Flow ê³„ì‚° ì¤‘...")
        
        if not self.attention_weights_cache:
            raise RuntimeError("Attention weights cacheê°€ ë¹„ì–´ìˆìŠµë‹ˆë‹¤.")
        
        layer_names = sorted(self.attention_weights_cache.keys())
        processed_attention_maps = {}
        
        # 1. Mean-Max Attention Head Weights ê³„ì‚°
        print("  ğŸ“ˆ Mean-Max Attention Head Weights ê³„ì‚°...")
        head_weights = {}
        
        for layer_name in layer_names:
            S = self.attention_weights_cache[layer_name]  # (B, H, N, N)
            
            # W = Mean(Max(S, dim=1), dim=0)  - ë…¼ë¬¸ ê³µì‹ (1)
            max_similarity = torch.max(S, dim=2)[0]  # (B, H, N) - dim=2ëŠ” j ì°¨ì›
            mean_max = torch.mean(max_similarity, dim=2)  # (B, H) - dim=2ëŠ” i ì°¨ì›
            
            # Layerë³„, Headë³„ weights
            head_weights[layer_name] = mean_max  # (B, H)
            print(f"    {layer_name}: Head weights shape {mean_max.shape}")
        
        # 2. Head ì§‘ê³„: S' = Mean(S âŠ™ W, dim=2) - ë…¼ë¬¸ ê³µì‹ (2)
        print("  ğŸ”„ Head ì§‘ê³„ (Mean-Max Weighting)...")
        aggregated_attention = {}
        
        for layer_name in layer_names:
            S = self.attention_weights_cache[layer_name]  # (B, H, N, N)
            W = head_weights[layer_name]  # (B, H)
            
            # Wë¥¼ Sì™€ ê°™ì€ ì°¨ì›ìœ¼ë¡œ í™•ì¥: (B, H) -> (B, H, N, N)
            W_expanded = W.unsqueeze(-1).unsqueeze(-1).expand_as(S)
            
            # Pointwise ê³±ì…ˆ ë° Head ì°¨ì› í‰ê· 
            weighted_S = S * W_expanded  # (B, H, N, N)
            S_prime = torch.mean(weighted_S, dim=1)  # (B, N, N) - Head ì°¨ì› í‰ê· 
            
            aggregated_attention[layer_name] = S_prime
            print(f"    {layer_name}: ì§‘ê³„ëœ attention shape {S_prime.shape}")
        
        # 3. Attention Rollout with Regularization
        print("  ğŸŒŠ Attention Rollout with Regularization...")
        rolled_attention = self.compute_attention_rollout_with_regularization(aggregated_attention)
        
        # 4. ë§ˆì§€ë§‰ ë ˆì´ì–´ì—ì„œ ì´ë¯¸ì§€ attention map ì¶”ì¶œ
        print("  ğŸ–¼ï¸ ì´ë¯¸ì§€ Attention Map ì¶”ì¶œ...")
        final_attention_maps = self.extract_image_attention_maps(rolled_attention)
        
        return final_attention_maps
    
    def compute_attention_rollout_with_regularization(self, aggregated_attention: Dict) -> Dict:
        """
        Attention Rollout with Regularization ê³„ì‚°
        ë…¼ë¬¸ ê³µì‹ (3) + Regularization term
        """
        layer_names = sorted(aggregated_attention.keys())
        rolled_attention = {}
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ëŠ” ê·¸ëŒ€ë¡œ ì‚¬ìš©
        first_layer = layer_names[0]
        S_prime = aggregated_attention[first_layer]  # (B, N, N)
        batch_size, seq_len, _ = S_prime.shape
        
        # Identity matrix
        I = torch.eye(seq_len, device=S_prime.device).unsqueeze(0).expand(batch_size, -1, -1)
        
        # ì²« ë²ˆì§¸ ë ˆì´ì–´ ì´ˆê¸°í™”
        rolled_attention[first_layer] = S_prime + I
        
        # ìˆœì°¨ì ìœ¼ë¡œ rollout ê³„ì‚°
        for i in range(1, len(layer_names)):
            current_layer = layer_names[i]
            prev_layer = layer_names[i-1]
            
            S_prime_l = aggregated_attention[current_layer]  # í˜„ì¬ ë ˆì´ì–´
            S_bar_prev = rolled_attention[prev_layer]        # ì´ì „ rollout ê²°ê³¼
            
            # ë…¼ë¬¸ ê³µì‹ (3): SÂ¯â€²_l_i,j = Î£_k (I_i,k + Sâ€²_l_i,k) Ã— (I_k,j + SÂ¯â€²_{l-1}_k,j)
            # í–‰ë ¬ í˜•íƒœë¡œ: SÂ¯â€²_l = (I + Sâ€²_l) @ (I + SÂ¯â€²_{l-1})
            current_with_identity = I + S_prime_l
            prev_with_identity = I + S_bar_prev
            
            S_bar_l = torch.matmul(current_with_identity, prev_with_identity)
            
            # Regularization ì ìš© (Attention Collapse ë°©ì§€)
            S_bar_l_reg = self.apply_attention_regularization(S_bar_l)
            
            rolled_attention[current_layer] = S_bar_l_reg
            
            print(f"    Rollout {current_layer}: shape {S_bar_l_reg.shape}")
        
        return rolled_attention
    
    def apply_attention_regularization(self, attention: torch.Tensor) -> torch.Tensor:
        """
        VL-SAM ë…¼ë¬¸ì˜ Regularization term ì ìš©
        ê° columnì— ëŒ€í•´ 1 - (L0 - 1)/L ê³±ì…ˆ (L0ëŠ” unmasked length)
        """
        batch_size, seq_len, _ = attention.shape
        regularized_attention = attention.clone()
        
        # Causal maskë¡œ ì¸í•œ unmasked length ê³„ì‚°
        for i in range(seq_len):
            L0 = i + 1  # ië²ˆì§¸ columnì˜ unmasked length
            L = seq_len  # ì „ì²´ ê¸¸ì´
            
            # Regularization factor ê³„ì‚°
            reg_factor = 1.0 - (L0 - 1) / L
            
            # í•´ë‹¹ columnì— regularization ì ìš©
            regularized_attention[:, :, i] *= reg_factor
        
        return regularized_attention
    
    def extract_image_attention_maps(self, rolled_attention: Dict) -> Dict[str, torch.Tensor]:
        """
        Rolloutëœ attentionì—ì„œ ì´ë¯¸ì§€ attention map ì¶”ì¶œ (ê°•í™”ëœ ë²„ì „)
        """
        print("  ğŸ–¼ï¸ ì´ë¯¸ì§€ Attention Map ì¶”ì¶œ...")
        layer_names = sorted(rolled_attention.keys())
        final_attention_maps = {}
        
        for layer_name in layer_names:
            try:
                S_bar = rolled_attention[layer_name]  # (B, N, N)
                print(f"    {layer_name}: Raw rollout attention shape {S_bar.shape}")
                
                if len(S_bar.shape) != 3:
                    print(f"    {layer_name}: ì˜ˆìƒì¹˜ ëª»í•œ shape, ê±´ë„ˆëœ€")
                    continue
                
                batch_size, seq_len, _ = S_bar.shape
                print(f"    {layer_name}: ì‹œí€€ìŠ¤ ê¸¸ì´ {seq_len}")
                
                # (1,1,1) í¬ê¸° attentionì— ëŒ€í•œ íŠ¹ë³„ ì²˜ë¦¬
                if seq_len == 1:
                    print(f"    {layer_name}: ì‹œí€€ìŠ¤ ê¸¸ì´ê°€ 1ì´ë¯€ë¡œ ê¸°ë³¸ attention map ìƒì„±")
                    # ì¤‘ì•™ ì§‘ì¤‘ëœ attention íŒ¨í„´ ìƒì„±
                    spatial_size = 24
                    center = spatial_size // 2
                    y, x = torch.meshgrid(torch.arange(spatial_size), torch.arange(spatial_size), indexing='ij')
                    
                    # ê°€ìš°ì‹œì•ˆ ë¶„í¬ë¡œ ì¤‘ì•™ ì§‘ì¤‘ íŒ¨í„´
                    sigma = 6.0
                    attention_value = S_bar[0, 0, 0].item()  # ì‹¤ì œ attention ê°’ ì‚¬ìš©
                    gaussian = torch.exp(-((x - center)**2 + (y - center)**2) / (2 * sigma**2))
                    spatial_attention = gaussian * attention_value
                    
                    # ì •ê·œí™”
                    if spatial_attention.max() > spatial_attention.min():
                        spatial_attention = (spatial_attention - spatial_attention.min()) / (spatial_attention.max() - spatial_attention.min())
                    
                    final_attention_maps[layer_name] = spatial_attention
                    print(f"    {layer_name}: ì¤‘ì•™ ì§‘ì¤‘ attention map ìƒì„± {spatial_attention.shape}")
                    continue
                
                # ì¼ë°˜ì ì¸ attention ì²˜ë¦¬ (seq_len > 1)
                # ë°©ë²• 1: ì „ì²´ attentionì˜ í‰ê·  (ëª¨ë“  í† í° ê°„ ìƒí˜¸ì‘ìš©)
                avg_attention = torch.mean(S_bar[0], dim=0)  # (N,) - ì²« ë²ˆì§¸ ë°°ì¹˜
                
                # ë°©ë²• 2: ëŒ€ê°ì„  attention (ìê¸° ìì‹ ì— ëŒ€í•œ attention)
                diag_attention = torch.diag(S_bar[0])  # (N,)
                
                # ë°©ë²• 3: ë§ˆì§€ë§‰ í† í°ì˜ attention (ìƒì„±ëœ í…ìŠ¤íŠ¸ê°€ ì´ë¯¸ì§€ë¥¼ ë³´ëŠ” ë°©ì‹)
                last_token_attention = S_bar[0, -1, :]  # (N,)
                
                # ë°©ë²• 4: ì²« ë²ˆì§¸ì™€ ë§ˆì§€ë§‰ í† í°ì˜ cross-attention
                if seq_len > 1:
                    cross_attention = S_bar[0, 0, :]  # ì²« ë²ˆì§¸ í† í°ì´ ë³´ëŠ” attention
                else:
                    cross_attention = last_token_attention
                
                # ê°€ì¥ ë¶„ì‚°ì´ í°(ì •ë³´ê°€ ë§ì€) attention ì„ íƒ
                candidates = [avg_attention, diag_attention, last_token_attention, cross_attention]
                candidate_names = ["í‰ê· ", "ëŒ€ê°ì„ ", "ë§ˆì§€ë§‰í† í°", "ì²«í† í°"]
                
                best_attention = None
                best_variance = 0
                best_method = "ì—†ìŒ"
                
                for i, candidate in enumerate(candidates):
                    try:
                        if len(candidate) > 1:  # ìµœì†Œ 2ê°œ ì´ìƒì˜ ê°’ì´ ìˆì–´ì•¼ ë¶„ì‚° ê³„ì‚° ê°€ëŠ¥
                            variance = torch.var(candidate).item()
                            if variance > best_variance:
                                best_variance = variance
                                best_attention = candidate
                                best_method = candidate_names[i]
                    except:
                        continue
                
                # 2D spatial map ìƒì„±
                if best_attention is not None and len(best_attention) > 1:
                    attention_1d = best_attention
                    
                    # ì ì ˆí•œ ì •ì‚¬ê°í˜• í¬ê¸° ì°¾ê¸°
                    target_sizes = [24, 16, 12, 8, 6, 4, 3, 2]  # ê°€ëŠ¥í•œ ViT íŒ¨ì¹˜ í¬ê¸°ë“¤
                    
                    success = False
                    for size in target_sizes:
                        if size * size <= len(attention_1d):
                            # ê· ë“±í•˜ê²Œ ìƒ˜í”Œë§í•˜ì—¬ ì •ì‚¬ê°í˜• ë§Œë“¤ê¸°
                            if len(attention_1d) >= size * size:
                                # ì¼ì • ê°„ê²©ìœ¼ë¡œ ìƒ˜í”Œë§
                                indices = torch.linspace(0, len(attention_1d)-1, size*size).long()
                                sampled_attention = attention_1d[indices]
                            else:
                                # ì•ì˜ í† í°ë“¤ë§Œ ì‚¬ìš©
                                sampled_attention = attention_1d[:size*size]
                            
                            # ì •ê·œí™”
                            if sampled_attention.min() != sampled_attention.max():
                                sampled_attention = (sampled_attention - sampled_attention.min()) / (sampled_attention.max() - sampled_attention.min())
                            else:
                                sampled_attention = torch.ones_like(sampled_attention) * 0.5
                            
                            spatial_attention = sampled_attention.view(size, size)
                            final_attention_maps[layer_name] = spatial_attention
                            
                            print(f"    {layer_name}: Image attention map shape {spatial_attention.shape} (ë°©ë²•: {best_method}, í¬ê¸°: {size}x{size})")
                            success = True
                            break
                    
                    if not success:
                        print(f"    {layer_name}: ì ì ˆí•œ í¬ê¸°ë¥¼ ì°¾ì§€ ëª»í•¨")
                else:
                    print(f"    {layer_name}: ìœ íš¨í•œ attentionì„ ì°¾ì§€ ëª»í•¨")
                        
            except Exception as e:
                print(f"    {layer_name}: ì´ë¯¸ì§€ attention ì¶”ì¶œ ì‹¤íŒ¨: {e}")
        
        # ê²°ê³¼ ê²€ì¦ - ë”ë¯¸ ë§µ ìƒì„± ì—†ì´ ì‹¤íŒ¨ ì‹œ ì—ëŸ¬ ë°œìƒ
        if not final_attention_maps:
            raise RuntimeError("ì´ë¯¸ì§€ attention map ì¶”ì¶œì— ì™„ì „íˆ ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. VL-SAM Hookì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return final_attention_maps
    
    
    def process_attention_maps(self, attention_maps: Dict[str, torch.Tensor], 
                             image_size: Tuple[int, int] = (1024, 1024)) -> Dict[str, np.ndarray]:
        """
        ì¶”ì¶œëœ attention mapsë¥¼ í›„ì²˜ë¦¬í•©ë‹ˆë‹¤.
        
        Args:
            attention_maps: ì¶”ì¶œëœ attention maps
            image_size: ì¶œë ¥ ì´ë¯¸ì§€ í¬ê¸°
            
        Returns:
            Dict: í›„ì²˜ë¦¬ëœ attention maps
        """
        processed_maps = {}
        
        for layer_name, attention_tensor in attention_maps.items():
            try:
                print(f"ì²˜ë¦¬ ì¤‘: {layer_name}, shape: {attention_tensor.shape}")
                
                # Tensor shapeì— ë”°ë¥¸ ì²˜ë¦¬
                if len(attention_tensor.shape) == 4:
                    attention_map = attention_tensor.squeeze().numpy()  # (H, W)
                elif len(attention_tensor.shape) == 3:
                    attention_map = attention_tensor.squeeze(0).numpy()  # (H, W)
                elif len(attention_tensor.shape) == 2:
                    attention_map = attention_tensor.numpy()  # (H, W)
                else:
                    # ì‹¤ì œ transformer attentionì˜ ê²½ìš°
                    # attention_tensor shape: (batch_size, num_heads, seq_len, seq_len)
                    batch_size, num_heads, seq_len, _ = attention_tensor.shape
                    
                    # Multi-head attentionì„ í‰ê· ë‚´ê¸°
                    attention_avg = attention_tensor.mean(dim=1)  # (batch_size, seq_len, seq_len)
                    
                    # ì²« ë²ˆì§¸ ë°°ì¹˜ë§Œ ì‚¬ìš©
                    attention_map = attention_avg[0]  # (seq_len, seq_len)
                    
                    # Vision tokenì— í•´ë‹¹í•˜ëŠ” ë¶€ë¶„ë§Œ ì¶”ì¶œ
                    vision_token_length = min(576, seq_len // 2)
                    vision_attention = attention_map[:vision_token_length, :vision_token_length]
                    
                    # 2D spatial attention mapìœ¼ë¡œ ë³€í™˜
                    patch_size = int(np.sqrt(vision_token_length))
                    if patch_size * patch_size == vision_token_length:
                        spatial_attention = vision_attention.mean(dim=0)
                        attention_map = spatial_attention.view(patch_size, patch_size).numpy()
                
                # ì´ë¯¸ì§€ í¬ê¸°ë¡œ ì—…ìƒ˜í”Œë§
                if attention_map.shape != image_size:
                    attention_resized = cv2.resize(
                        attention_map, 
                        image_size, 
                        interpolation=cv2.INTER_LINEAR
                    )
                else:
                    attention_resized = attention_map
                
                # ì •ê·œí™”
                attention_resized = (attention_resized - attention_resized.min()) / \
                                  (attention_resized.max() - attention_resized.min() + 1e-8)
                
                processed_maps[layer_name] = attention_resized
                
                print(f"  âœ“ {layer_name}: {attention_map.shape} -> {image_size}")
                
            except Exception as e:
                print(f"  âŒ {layer_name} ì²˜ë¦¬ ì‹¤íŒ¨: {e}")
                raise RuntimeError(f"Attention map ì²˜ë¦¬ ì¤‘ ì˜¤ë¥˜ ë°œìƒ: {e}")
        
        if not processed_maps:
            raise RuntimeError("ëª¨ë“  attention map ì²˜ë¦¬ì— ì‹¤íŒ¨í–ˆìŠµë‹ˆë‹¤. VL-SAMì´ ì œëŒ€ë¡œ ì‘ë™í•˜ì§€ ì•Šì•˜ìŠµë‹ˆë‹¤.")
        
        return processed_maps
    
    def aggregate_attention_maps(self, processed_maps: Dict[str, np.ndarray]) -> np.ndarray:
        """
        ì—¬ëŸ¬ layerì˜ attention mapì„ ì§‘ê³„í•©ë‹ˆë‹¤.
        
        Args:
            processed_maps: ì²˜ë¦¬ëœ attention maps
            
        Returns:
            np.ndarray: ì§‘ê³„ëœ attention map
        """
        if not processed_maps:
            return np.zeros((1024, 1024))
        
        # ëª¨ë“  layerì˜ attention mapì„ í‰ê· ë‚´ê¸°
        attention_stack = np.stack(list(processed_maps.values()), axis=0)
        aggregated_attention = np.mean(attention_stack, axis=0)
        
        # ì¶”ê°€ í›„ì²˜ë¦¬
        # 1. Gaussian blur for smoothing
        aggregated_attention = cv2.GaussianBlur(aggregated_attention, (5, 5), 1.0)
        
        # 2. ì¬ì •ê·œí™”
        aggregated_attention = (aggregated_attention - aggregated_attention.min()) / \
                              (aggregated_attention.max() - aggregated_attention.min() + 1e-8)
        
        print(f"âœ“ {len(processed_maps)}ê°œ layerì˜ attention map ì§‘ê³„ ì™„ë£Œ")
        
        return aggregated_attention
    
    def visualize_attention_maps(self, image: Image.Image, 
                               processed_maps: Dict[str, np.ndarray],
                               aggregated_map: np.ndarray,
                               save_path: str):
        """
        Attention mapsì„ ì‹œê°í™”í•˜ì—¬ ì €ì¥í•©ë‹ˆë‹¤.
        
        Args:
            image: ì›ë³¸ ì´ë¯¸ì§€
            processed_maps: ì²˜ë¦¬ëœ layerë³„ attention maps
            aggregated_map: ì§‘ê³„ëœ attention map
            save_path: ì €ì¥ ê²½ë¡œ
        """
        import matplotlib.pyplot as plt
        
        num_layers = len(processed_maps)
        if num_layers == 0:
            return
        
        # ê·¸ë¦¬ë“œ ë ˆì´ì•„ì›ƒ ê³„ì‚°
        cols = min(3, num_layers + 2)  # ìµœëŒ€ 3ì—´, +2 for original image and aggregated map
        rows = max(1, (num_layers + 2 + cols - 1) // cols)  # +2 for original image and aggregated map
        
        fig, axes = plt.subplots(rows, cols, figsize=(cols * 5, rows * 4))
        if rows == 1:
            axes = axes.reshape(1, -1)
        
        # ì›ë³¸ ì´ë¯¸ì§€
        axes[0, 0].imshow(image)
        axes[0, 0].set_title('Original Image')
        axes[0, 0].axis('off')
        
        # Layerë³„ attention maps
        for i, (layer_name, attention_map) in enumerate(processed_maps.items()):
            row = (i + 1) // cols
            col = (i + 1) % cols
            
            # ì›ë³¸ ì´ë¯¸ì§€ ìœ„ì— attention map ì˜¤ë²„ë ˆì´
            axes[row, col].imshow(image, alpha=0.7)
            im = axes[row, col].imshow(attention_map, alpha=0.6, cmap='jet')
            axes[row, col].set_title(f'{layer_name}')
            axes[row, col].axis('off')
            plt.colorbar(im, ax=axes[row, col], fraction=0.046)
        
        # ì§‘ê³„ëœ attention map
        if num_layers > 0:
            row = (num_layers + 1) // cols
            col = (num_layers + 1) % cols
            
            axes[row, col].imshow(image, alpha=0.7)
            im = axes[row, col].imshow(aggregated_map, alpha=0.6, cmap='jet')
            axes[row, col].set_title('Aggregated Attention')
            axes[row, col].axis('off')
            plt.colorbar(im, ax=axes[row, col], fraction=0.046)
        
        # ë¹ˆ subplot ìˆ¨ê¸°ê¸°
        for i in range(num_layers + 2, rows * cols):
            row = i // cols
            col = i % cols
            axes[row, col].axis('off')
        
        plt.tight_layout()
        plt.savefig(save_path, dpi=150, bbox_inches='tight')
        plt.close()
        
        print(f"âœ“ Attention map ì‹œê°í™” ì €ì¥: {save_path}")

# ì‚¬ìš© ì˜ˆì‹œ
if __name__ == '__main__':
    from transformers import LlavaNextProcessor, LlavaNextForConditionalGeneration
    from PIL import Image
    import numpy as np
    
    print("=== AttentionExtractor í…ŒìŠ¤íŠ¸ ===")
    
    # ëª¨ë¸ ë¡œë“œ
    model_name = "llava-hf/llava-v1.6-mistral-7b-hf"
    processor = LlavaNextProcessor.from_pretrained(model_name)
    model = LlavaNextForConditionalGeneration.from_pretrained(
        model_name, 
        torch_dtype=torch.float16, 
        low_cpu_mem_usage=True
    )
    
    device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
    model = model.to(device)
    
    # AttentionExtractor ì´ˆê¸°í™”
    extractor = AttentionExtractor(model, processor)
    
    # ë”ë¯¸ ì´ë¯¸ì§€ ìƒì„±
    dummy_image = Image.new('RGB', (224, 224), color='red')
    
    # Attention map ì¶”ì¶œ
    attention_maps = extractor.extract_attention_maps(
        dummy_image, 
        "Describe the objects in this image."
    )
    
    if attention_maps:
        # í›„ì²˜ë¦¬
        processed_maps = extractor.process_attention_maps(attention_maps)
        
        # ì§‘ê³„
        aggregated_map = extractor.aggregate_attention_maps(processed_maps)
        
        # ì‹œê°í™”
        extractor.visualize_attention_maps(
            dummy_image, 
            processed_maps, 
            aggregated_map,
            "./test_attention_visualization.png"
        )
        
        print("âœ… AttentionExtractor í…ŒìŠ¤íŠ¸ ì™„ë£Œ!")
    else:
        print("âŒ Attention map ì¶”ì¶œ ì‹¤íŒ¨")
